{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.explain import Explainer, CaptumExplainer,ModelConfig,GNNExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"../../data/processed/graph_data_nohubs/\"\n",
    "models_folder = \"../../data/models/\"\n",
    "experiments_folder = \"../../data/experiments/design_space_experiment/\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from src.models.base_model import base_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import SAGEConv,GATConv, to_hetero\n",
    "\n",
    "class inner_product_decoder(torch.nn.Module):\n",
    "    def forward(self,x_source,x_target,edge_index,apply_sigmoid=True):\n",
    "        nodes_src = x_source[edge_index[0]]\n",
    "        nodes_trg = x_target[edge_index[1]]\n",
    "        pred = (nodes_src * nodes_trg).sum(dim=-1)\n",
    "\n",
    "        if apply_sigmoid:\n",
    "            pred = torch.sigmoid(pred)\n",
    "\n",
    "        return pred\n",
    "\n",
    "class base_message_layer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model_params,hidden_layer=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Currently SageConv or GATConv, might have to modify this to support other Convs\n",
    "        conv_type = model_params[\"conv_type\"]\n",
    "        self.conv = layer_dict[conv_type]((-1,-1), model_params[\"hidden_channels\"],aggr=model_params[\"micro_aggregation\"],add_self_loops=False)\n",
    "        self.normalize = model_params[\"L2_norm\"]\n",
    "\n",
    "        post_conv_modules = []\n",
    "        if model_params[\"batch_norm\"]:\n",
    "            bn = torch.nn.BatchNorm1d(model_params[\"hidden_channels\"])\n",
    "            post_conv_modules.append(bn)\n",
    "        \n",
    "        if model_params[\"dropout\"] > 0:    \n",
    "            dropout = torch.nn.Dropout(p=model_params[\"dropout\"])\n",
    "            post_conv_modules.append(dropout)\n",
    "        \n",
    "        # No activation on final embedding layer\n",
    "        if hidden_layer:\n",
    "            activation = model_params[\"activation\"]()\n",
    "            post_conv_modules.append(activation)\n",
    "        \n",
    "        self.post_conv = torch.nn.Sequential(*post_conv_modules)\n",
    "\n",
    "    def forward(self, x:dict, edge_index:dict) -> dict:\n",
    "        x = self.conv(x,edge_index)\n",
    "        x = self.post_conv(x)\n",
    "        if self.normalize:\n",
    "            x = torch.nn.functional.normalize(x,2,-1)\n",
    "        return x\n",
    "\n",
    "class multilayer_message_passing(torch.nn.Module):\n",
    "    #TODO: consider input and output dims with skipcat. Currently the two supported convs auto-detect dimensions. Might have to modify this if i add more convs in the future.\n",
    "    def __init__(self,num_layers,model_params,metadata):\n",
    "        super().__init__()\n",
    "\n",
    "        self.skip = model_params[\"layer_connectivity\"]\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            hidden_layer = i != self.num_layers-1\n",
    "            layer = to_hetero(base_message_layer(model_params,hidden_layer),metadata,model_params[\"macro_aggregation\"])\n",
    "            self.add_module(f\"Layer_{i}\",layer)\n",
    "    \n",
    "    def hetero_skipsum(self,x: dict, x_i:dict) -> dict:\n",
    "        x_transformed = {}\n",
    "        for key,x_val in x.items():\n",
    "            x_i_val = x_i[key]\n",
    "            transformed_val = x_val + x_i_val\n",
    "            x_transformed[key] = transformed_val\n",
    "\n",
    "        return x_transformed\n",
    "\n",
    "    def hetero_skipcat(self,x: dict, x_i:dict) -> dict:\n",
    "        x_transformed = {}\n",
    "        for key,x_val in x.items():\n",
    "            x_i_val = x_i[key]\n",
    "            transformed_val = torch.cat([x_val,x_i_val],dim=-1)\n",
    "            x_transformed[key] = transformed_val\n",
    "\n",
    "        return x_transformed\n",
    "    \n",
    "    def forward(self, x:dict, edge_index:dict) -> dict:\n",
    "        for i, layer in enumerate(self.children()):\n",
    "            x_i = x\n",
    "            x = layer(x,edge_index)\n",
    "            if self.skip == \"skipsum\":\n",
    "                x = self.hetero_skipsum(x,x_i)\n",
    "            elif self.skip == \"skipcat\" and i < self.num_layers -1:\n",
    "                x = self.hetero_skipcat(x,x_i)\n",
    "        \n",
    "        return x \n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self,num_layers,in_dim,out_dim,model_params,hidden_dim=None):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dim = out_dim if hidden_dim is None else hidden_dim\n",
    "        \n",
    "        modules = []\n",
    "        if num_layers == 1:\n",
    "            modules.append(torch.nn.Linear(in_dim,out_dim))\n",
    "        else:\n",
    "            for i in range(num_layers):\n",
    "                final_layer = i == num_layers-1\n",
    "                first_layer = i == 0\n",
    "                if first_layer:\n",
    "                    modules.append(torch.nn.Linear(in_dim,hidden_dim))\n",
    "                    modules.append(model_params[\"activation\"]())\n",
    "                elif final_layer:\n",
    "                    modules.append(torch.nn.Linear(hidden_dim,out_dim))\n",
    "                else:\n",
    "                    modules.append(torch.nn.Linear(hidden_dim,hidden_dim))\n",
    "                    modules.append(model_params[\"activation\"]())\n",
    "        \n",
    "        self.model = torch.nn.Sequential(*modules)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "class base_encoder(torch.nn.Module):\n",
    "    def __init__(self,model_params,metadata):\n",
    "        super().__init__()\n",
    "\n",
    "        self.has_pre_mlp = model_params[\"pre_process_layers\"] > 0\n",
    "        self.has_post_mlp = model_params[\"post_process_layers\"] > 0\n",
    "\n",
    "        if self.has_pre_mlp:\n",
    "            self.pre_mlp = to_hetero(MLP(model_params[\"pre_process_layers\"],model_params[\"feature_dim\"],model_params[\"hidden_channels\"],model_params),metadata)\n",
    "        \n",
    "        self.message_passing = multilayer_message_passing(model_params[\"msg_passing_layers\"],model_params,metadata)\n",
    "\n",
    "        if self.has_post_mlp:\n",
    "            self.post_mlp = to_hetero(MLP(model_params[\"post_process_layers\"],model_params[\"hidden_channels\"],model_params[\"hidden_channels\"],model_params),metadata)\n",
    "    \n",
    "    def forward(self,x:dict,edge_index:dict) -> dict :\n",
    "        if self.has_pre_mlp:\n",
    "            x = self.pre_mlp(x)\n",
    "\n",
    "        x = self.message_passing(x,edge_index)\n",
    "        \n",
    "        if self.has_post_mlp:\n",
    "            x = self.post_mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class base_explainable_model(torch.nn.Module):\n",
    "    def __init__(self, model_params,metadata,supervision_types=[('gene_protein', 'gda', 'disease')]):\n",
    "        super().__init__()\n",
    "\n",
    "        default_model_params = {\n",
    "            \"hidden_channels\":32,\n",
    "            \"conv_type\":\"SAGEConv\",\n",
    "            \"batch_norm\": True,\n",
    "            \"dropout\":0,\n",
    "            \"activation\":torch.nn.LeakyReLU,\n",
    "            \"micro_aggregation\":\"mean\",\n",
    "            \"macro_aggregation\":\"mean\",\n",
    "            \"layer_connectivity\":None,\n",
    "            \"L2_norm\":False,\n",
    "            \"feature_dim\": 10,\n",
    "            \"pre_process_layers\":0,\n",
    "            \"msg_passing_layers\":2,\n",
    "            \"post_process_layers\":0,\n",
    "        }\n",
    "        \n",
    "        for arg in default_model_params:\n",
    "            if arg not in model_params:\n",
    "                model_params[arg] = default_model_params[arg]\n",
    "        \n",
    "        self.encoder = base_encoder(model_params,metadata)\n",
    "        self.decoder = inner_product_decoder()\n",
    "        self.loss_fn = torch.nn.BCELoss()\n",
    "        self.supervision_types = supervision_types\n",
    "    \n",
    "    def decode(self,x:dict,edge_label_index):\n",
    "        # pred_dict = {}\n",
    "        # for edge_type in self.supervision_types:\n",
    "        #     edge_index = edge_label_index[edge_type]\n",
    "\n",
    "        #     src_type = edge_type[0]\n",
    "        #     trg_type = edge_type[2]\n",
    "\n",
    "        #     x_src = x[src_type]\n",
    "        #     x_trg = x[trg_type]\n",
    "\n",
    "        #     pred = self.decoder(x_src,x_trg,edge_index)\n",
    "\n",
    "        #     pred_dict[edge_type] = pred\n",
    "        \n",
    "        # final_pred = pred_dict[self.supervision_types[0]]\n",
    "        # # final_pred = final_pred.reshape([1,final_pred.shape[0]])\n",
    "        edge_index = edge_label_index\n",
    "        edge_type = self.supervision_types[0]\n",
    "\n",
    "        src_type = edge_type[0]\n",
    "        trg_type = edge_type[2]\n",
    "\n",
    "        x_src = x[src_type]\n",
    "        x_trg = x[trg_type]\n",
    "\n",
    "        pred = self.decoder(x_src,x_trg,edge_index)\n",
    "        \n",
    "        final_pred = pred\n",
    "        # final_pred = final_pred.reshape([1,final_pred.shape[0]])\n",
    "        return final_pred\n",
    "    \n",
    "    def encode(self,data):\n",
    "        x = data.x_dict\n",
    "        adj_t = data.adj_t_dict\n",
    "\n",
    "        encodings = self.encoder(x,adj_t)\n",
    "        return encodings\n",
    "    \n",
    "    def forward(self,x:dict,adj_t:dict,edge_label_index:dict):\n",
    "        # x = data.x_dict\n",
    "        # adj_t = data.adj_t_dict\n",
    "        # edge_label_index = data.edge_label_index_dict\n",
    "\n",
    "        x = self.encoder(x,adj_t)\n",
    "        pred = self.decode(x,edge_label_index)\n",
    "        return pred\n",
    "    \n",
    "    def loss(self, prediction_dict, label_dict):\n",
    "        loss = 0\n",
    "        num_types = len(prediction_dict.keys())\n",
    "        for edge_type,pred in prediction_dict.items():\n",
    "            y = label_dict[edge_type]\n",
    "            loss += self.loss_fn(pred, y.type(pred.dtype))\n",
    "        return loss/num_types\n",
    "\n",
    "layer_dict = {\n",
    "    \"GATConv\":GATConv,\n",
    "    \"SAGEConv\":SAGEConv\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def load_data(folder_path,load_test = False):\n",
    "    if load_test:\n",
    "        names = [\"train\",\"validation\",\"test\"]\n",
    "    else:\n",
    "        names = [\"train\",\"validation\"]\n",
    "    datasets = []\n",
    "    for name in names:\n",
    "        path = folder_path+name+\".pt\"\n",
    "        datasets.append(torch.load(path))\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "def initialize_features(data,feature,dim,inplace=False):\n",
    "    if inplace:\n",
    "        data_object = data\n",
    "    else:\n",
    "        data_object = copy.copy(data)\n",
    "    for nodetype, store in data_object.node_items():\n",
    "        if feature == \"random\":\n",
    "            data_object[nodetype].x = torch.rand(store[\"num_nodes\"],dim)\n",
    "        if feature == \"ones\":\n",
    "            data_object[nodetype].x = torch.ones(store[\"num_nodes\"],dim)\n",
    "    return data_object\n",
    "\n",
    "def load_model(state_dict,params,metadata):\n",
    "    model = base_model(params,metadata,supervision_types=[('gene_protein', 'gda', 'disease')])\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "def load_experiment(eid:int,date:str,metadata:tuple) -> tuple:\n",
    "    \"\"\"Returns tuple (model,params).\n",
    "    date format: d_m_y\"\"\"\n",
    "    df_path = f\"{experiments_folder}experiment_{date}.parquet\"\n",
    "    weights_path = f\"{experiments_folder}experiment_{eid}_{date}__.pth\"\n",
    "\n",
    "    df = pd.read_parquet(df_path)\n",
    "    #TODO: this is only temporal, remove after fix\n",
    "    df[\"conv_type\"] = df.conv_type.apply(lambda x: x.split(\".\")[-1].rstrip(\"\\'>\"))\n",
    "    df[\"activation\"] = torch.nn.LeakyReLU\n",
    "    params = df.loc[eid].to_dict()\n",
    "    weights = torch.load(weights_path,map_location=torch.device(device))\n",
    "\n",
    "    model = base_explainable_model(params,metadata)\n",
    "    model.load_state_dict(weights)\n",
    "\n",
    "    return model,params\n",
    "\n",
    "def load_node_csv(path, index_col,type_col, **kwargs):\n",
    "    \"\"\"Returns node dataframe and a dict of mappings for each node type. \n",
    "    Each mapping maps from original df index to \"heterodata index\" { node_type : { dataframe_index : heterodata_index}}\"\"\"\n",
    "    df = pd.read_csv(path, **kwargs,index_col=index_col)\n",
    "    node_types = df[type_col].unique()\n",
    "    mappings_dict = dict()\n",
    "    for node_type in node_types:\n",
    "        mapping = {index: i for i, index in enumerate(df[df[type_col] == node_type].index.unique())}\n",
    "        mappings_dict[node_type] = mapping\n",
    "\n",
    "    return df,mappings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,val_data = load_data(data_folder+\"split_dataset/\")\n",
    "\n",
    "eid = 34\n",
    "date = \"18_04_23\"\n",
    "model,params = load_experiment(eid,date,train_data.metadata())\n",
    "\n",
    "train_data = initialize_features(train_data,params[\"feature_type\"],params[\"feature_dim\"])\n",
    "val_data = initialize_features(train_data,params[\"feature_type\"],params[\"feature_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    aver = model(train_data.x_dict,train_data.edge_index_dict,train_data[\"gene_protein\",\"disease\"].edge_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3982, 0.7912, 0.8521,  ..., 0.4634, 0.0412, 0.0242])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data.x_dict\n",
    "adj_t = train_data.adj_t_dict\n",
    "edge_index = train_data.edge_index_dict\n",
    "edge_label_index = train_data[\"gene_protein\",\"disease\"].edge_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "allow_unused = True\n",
    "explainer = Explainer(\n",
    "    model=model,\n",
    "    algorithm=CaptumExplainer('IntegratedGradients'),\n",
    "    explanation_type='model',\n",
    "    model_config=dict(\n",
    "        mode='binary_classification',\n",
    "        task_level='edge',\n",
    "        return_type='probs',\n",
    "    ),\n",
    "    node_mask_type='attributes',\n",
    "    edge_mask_type='object'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2929,  6037,  4913,  ...,  3391, 15286,  7687],\n",
       "        [  870,  4171,  4728,  ...,  7081,  7141, 11498]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"gene_protein\",\"disease\"].edge_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = explainer.get_prediction(x,adj_t,edge_label_index)\n",
    "target = explainer.get_target(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 26884])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_label_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'IntegratedGradients' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb#X65sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m explainer(x,edge_index,edge_label_index\u001b[39m=\u001b[39;49medge_label_index)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/explain/explainer.py:198\u001b[0m, in \u001b[0;36mExplainer.__call__\u001b[0;34m(self, x, edge_index, target, index, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtraining\n\u001b[1;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n\u001b[0;32m--> 198\u001b[0m explanation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malgorithm(\n\u001b[1;32m    199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel,\n\u001b[1;32m    200\u001b[0m     x,\n\u001b[1;32m    201\u001b[0m     edge_index,\n\u001b[1;32m    202\u001b[0m     target\u001b[39m=\u001b[39;49mtarget,\n\u001b[1;32m    203\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m    204\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    205\u001b[0m )\n\u001b[1;32m    207\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain(training)\n\u001b[1;32m    209\u001b[0m \u001b[39m# Add explainer objectives to the `Explanation` object:\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/explain/algorithm/captum_explainer.py:153\u001b[0m, in \u001b[0;36mCaptumExplainer.forward\u001b[0;34m(self, model, x, edge_index, target, index, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m     metadata \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     captum_model \u001b[39m=\u001b[39m CaptumModel(model, mask_type, index)\n\u001b[0;32m--> 153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattribution_method \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattribution_method(captum_model)\n\u001b[1;32m    155\u001b[0m \u001b[39m# In captum, the target is the index for which\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[39m# the attribution is computed.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_config\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m ModelMode\u001b[39m.\u001b[39mregression:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'IntegratedGradients' object is not callable"
     ]
    }
   ],
   "source": [
    "explainer(x,edge_index,edge_label_index=edge_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'IntegratedGradients' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb#X66sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m explainer\u001b[39m.\u001b[39;49malgorithm\u001b[39m.\u001b[39;49mattribution_method()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'IntegratedGradients' object is not callable"
     ]
    }
   ],
   "source": [
    "explainer.algorithm.attribution_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'IntegratedGradients' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb#X64sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m explainer\u001b[39m.\u001b[39;49malgorithm(model,x,edge_index,edge_label_index\u001b[39m=\u001b[39;49medge_label_index,target\u001b[39m=\u001b[39;49mtarget,index\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mtensor([\u001b[39m1\u001b[39;49m]))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/explain/algorithm/captum_explainer.py:153\u001b[0m, in \u001b[0;36mCaptumExplainer.forward\u001b[0;34m(self, model, x, edge_index, target, index, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m     metadata \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     captum_model \u001b[39m=\u001b[39m CaptumModel(model, mask_type, index)\n\u001b[0;32m--> 153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattribution_method \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattribution_method(captum_model)\n\u001b[1;32m    155\u001b[0m \u001b[39m# In captum, the target is the index for which\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[39m# the attribution is computed.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_config\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m ModelMode\u001b[39m.\u001b[39mregression:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'IntegratedGradients' object is not callable"
     ]
    }
   ],
   "source": [
    "explainer.algorithm(model,x,edge_index,edge_label_index=edge_label_index,target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.explain.algorithm.captum import CaptumHeteroModel\n",
    "from torch_geometric.nn import to_captum_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "captum_model = to_captum_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<captum.attr._core.integrated_gradients.IntegratedGradients at 0x7f00e998d5a0>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explainer.algorithm.attribution_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'IntegratedGradients' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m explainer\u001b[39m.\u001b[39;49malgorithm\u001b[39m.\u001b[39;49mattribution_method()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'IntegratedGradients' object is not callable"
     ]
    }
   ],
   "source": [
    "explainer.algorithm.attribution_method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explainer.algorithm.supports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Explainer.__call__() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m explainer(x,adj_t,edge_index)\n",
      "\u001b[0;31mTypeError\u001b[0m: Explainer.__call__() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "explainer(x,adj_t,edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Tensor target dimension torch.Size([1, 26884]) is not valid. torch.Size([1, 26884])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m explainer(x,edge_index,edge_label_index\u001b[39m=\u001b[39;49medge_label_index)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/explain/explainer.py:198\u001b[0m, in \u001b[0;36mExplainer.__call__\u001b[0;34m(self, x, edge_index, target, index, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtraining\n\u001b[1;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n\u001b[0;32m--> 198\u001b[0m explanation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malgorithm(\n\u001b[1;32m    199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel,\n\u001b[1;32m    200\u001b[0m     x,\n\u001b[1;32m    201\u001b[0m     edge_index,\n\u001b[1;32m    202\u001b[0m     target\u001b[39m=\u001b[39;49mtarget,\n\u001b[1;32m    203\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m    204\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    205\u001b[0m )\n\u001b[1;32m    207\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain(training)\n\u001b[1;32m    209\u001b[0m \u001b[39m# Add explainer objectives to the `Explanation` object:\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/explain/algorithm/captum_explainer.py:162\u001b[0m, in \u001b[0;36mCaptumExplainer.forward\u001b[0;34m(self, model, x, edge_index, target, index, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     target \u001b[39m=\u001b[39m target[index]\n\u001b[0;32m--> 162\u001b[0m attributions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattribution_method\u001b[39m.\u001b[39;49mattribute(\n\u001b[1;32m    163\u001b[0m     inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[1;32m    164\u001b[0m     target\u001b[39m=\u001b[39;49mtarget,\n\u001b[1;32m    165\u001b[0m     additional_forward_args\u001b[39m=\u001b[39;49madd_forward_args,\n\u001b[1;32m    166\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs,\n\u001b[1;32m    167\u001b[0m )\n\u001b[1;32m    169\u001b[0m node_mask, edge_mask \u001b[39m=\u001b[39m convert_captum_output(\n\u001b[1;32m    170\u001b[0m     attributions,\n\u001b[1;32m    171\u001b[0m     mask_type,\n\u001b[1;32m    172\u001b[0m     metadata,\n\u001b[1;32m    173\u001b[0m )\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mdict\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/captum/log/__init__.py:42\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/captum/attr/_core/integrated_gradients.py:274\u001b[0m, in \u001b[0;36mIntegratedGradients.attribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[39mif\u001b[39;00m internal_batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m     num_examples \u001b[39m=\u001b[39m inputs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 274\u001b[0m     attributions \u001b[39m=\u001b[39m _batch_attribution(\n\u001b[1;32m    275\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    276\u001b[0m         num_examples,\n\u001b[1;32m    277\u001b[0m         internal_batch_size,\n\u001b[1;32m    278\u001b[0m         n_steps,\n\u001b[1;32m    279\u001b[0m         inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[1;32m    280\u001b[0m         baselines\u001b[39m=\u001b[39;49mbaselines,\n\u001b[1;32m    281\u001b[0m         target\u001b[39m=\u001b[39;49mtarget,\n\u001b[1;32m    282\u001b[0m         additional_forward_args\u001b[39m=\u001b[39;49madditional_forward_args,\n\u001b[1;32m    283\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    284\u001b[0m     )\n\u001b[1;32m    285\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     attributions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_attribute(\n\u001b[1;32m    287\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    288\u001b[0m         baselines\u001b[39m=\u001b[39mbaselines,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    292\u001b[0m         method\u001b[39m=\u001b[39mmethod,\n\u001b[1;32m    293\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/captum/attr/_utils/batching.py:78\u001b[0m, in \u001b[0;36m_batch_attribution\u001b[0;34m(attr_method, num_examples, internal_batch_size, n_steps, include_endpoint, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m step_sizes \u001b[39m=\u001b[39m full_step_sizes[start_step:end_step]\n\u001b[1;32m     77\u001b[0m alphas \u001b[39m=\u001b[39m full_alphas[start_step:end_step]\n\u001b[0;32m---> 78\u001b[0m current_attr \u001b[39m=\u001b[39m attr_method\u001b[39m.\u001b[39;49m_attribute(\n\u001b[1;32m     79\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, n_steps\u001b[39m=\u001b[39;49mbatch_steps, step_sizes_and_alphas\u001b[39m=\u001b[39;49m(step_sizes, alphas)\n\u001b[1;32m     80\u001b[0m )\n\u001b[1;32m     82\u001b[0m \u001b[39mif\u001b[39;00m total_attr \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     total_attr \u001b[39m=\u001b[39m current_attr\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/captum/attr/_core/integrated_gradients.py:351\u001b[0m, in \u001b[0;36mIntegratedGradients._attribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, step_sizes_and_alphas)\u001b[0m\n\u001b[1;32m    348\u001b[0m expanded_target \u001b[39m=\u001b[39m _expand_target(target, n_steps)\n\u001b[1;32m    350\u001b[0m \u001b[39m# grads: dim -> (bsz * #steps x inputs[0].shape[1:], ...)\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m grads \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgradient_func(\n\u001b[1;32m    352\u001b[0m     forward_fn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_func,\n\u001b[1;32m    353\u001b[0m     inputs\u001b[39m=\u001b[39;49mscaled_features_tpl,\n\u001b[1;32m    354\u001b[0m     target_ind\u001b[39m=\u001b[39;49mexpanded_target,\n\u001b[1;32m    355\u001b[0m     additional_forward_args\u001b[39m=\u001b[39;49minput_additional_args,\n\u001b[1;32m    356\u001b[0m )\n\u001b[1;32m    358\u001b[0m \u001b[39m# flattening grads so that we can multilpy it with step-size\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[39m# calling contiguous to avoid `memory whole` problems\u001b[39;00m\n\u001b[1;32m    360\u001b[0m scaled_grads \u001b[39m=\u001b[39m [\n\u001b[1;32m    361\u001b[0m     grad\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(n_steps, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    362\u001b[0m     \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mtensor(step_sizes)\u001b[39m.\u001b[39mview(n_steps, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(grad\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    363\u001b[0m     \u001b[39mfor\u001b[39;00m grad \u001b[39min\u001b[39;00m grads\n\u001b[1;32m    364\u001b[0m ]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/captum/_utils/gradient.py:112\u001b[0m, in \u001b[0;36mcompute_gradients\u001b[0;34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mComputes gradients of the output with respect to inputs for an\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39marbitrary forward function.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39m                arguments) if no additional arguments are required\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    111\u001b[0m     \u001b[39m# runs forward pass\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m     outputs \u001b[39m=\u001b[39m _run_forward(forward_fn, inputs, target_ind, additional_forward_args)\n\u001b[1;32m    113\u001b[0m     \u001b[39massert\u001b[39;00m outputs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnumel() \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m, (\n\u001b[1;32m    114\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTarget not provided when necessary, cannot\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m take gradient with respect to multiple outputs.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    117\u001b[0m     \u001b[39m# torch.unbind(forward_out) is a list of scalar tensor tuples and\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39m# contains batch_size * #steps elements\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/captum/_utils/common.py:487\u001b[0m, in \u001b[0;36m_run_forward\u001b[0;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[1;32m    480\u001b[0m additional_forward_args \u001b[39m=\u001b[39m _format_additional_forward_args(additional_forward_args)\n\u001b[1;32m    482\u001b[0m output \u001b[39m=\u001b[39m forward_func(\n\u001b[1;32m    483\u001b[0m     \u001b[39m*\u001b[39m(\u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39madditional_forward_args)\n\u001b[1;32m    484\u001b[0m     \u001b[39mif\u001b[39;00m additional_forward_args \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    485\u001b[0m     \u001b[39melse\u001b[39;00m inputs\n\u001b[1;32m    486\u001b[0m )\n\u001b[0;32m--> 487\u001b[0m \u001b[39mreturn\u001b[39;00m _select_targets(output, target)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/captum/_utils/common.py:506\u001b[0m, in \u001b[0;36m_select_targets\u001b[0;34m(output, target)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mgather(output, \u001b[39m1\u001b[39m, target\u001b[39m.\u001b[39mreshape(\u001b[39mlen\u001b[39m(output), \u001b[39m1\u001b[39m))\n\u001b[1;32m    505\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 506\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    507\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTensor target dimension \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m is not valid. \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m             \u001b[39m%\u001b[39m (target\u001b[39m.\u001b[39mshape, output\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    509\u001b[0m         )\n\u001b[1;32m    510\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(target, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    511\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(target) \u001b[39m==\u001b[39m num_examples, \u001b[39m\"\u001b[39m\u001b[39mTarget list length does not match output!\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Tensor target dimension torch.Size([1, 26884]) is not valid. torch.Size([1, 26884])"
     ]
    }
   ],
   "source": [
    "explainer(x,edge_index,edge_label_index=edge_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'x_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_config \u001b[39m=\u001b[39m ModelConfig(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_classification\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     task_level\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39medge\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     return_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m explainer \u001b[39m=\u001b[39m Explainer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     model,  \u001b[39m# It is assumed that model outputs a single tensor.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     algorithm\u001b[39m=\u001b[39mCaptumExplainer(\u001b[39m'\u001b[39m\u001b[39mIntegratedGradients\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     model_config \u001b[39m=\u001b[39m model_config\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m hetero_explanation \u001b[39m=\u001b[39m explainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     train_data\u001b[39m.\u001b[39;49mx_dict,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     train_data\u001b[39m.\u001b[39;49medge_index_dict,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     index\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mtensor([\u001b[39m1\u001b[39;49m, \u001b[39m3\u001b[39;49m]),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(hetero_explanation\u001b[39m.\u001b[39medge_mask_dict)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/testing_explainers.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(hetero_explanation\u001b[39m.\u001b[39mnode_mask_dict)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/explain/explainer.py:192\u001b[0m, in \u001b[0;36mExplainer.__call__\u001b[0;34m(self, x, edge_index, target, index, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[39mif\u001b[39;00m target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    190\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m\u001b[39m should not be provided for the explanation \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtype \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexplanation_type\u001b[39m.\u001b[39mvalue\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 192\u001b[0m     prediction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_prediction(x, edge_index, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    193\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_target(prediction)\n\u001b[1;32m    195\u001b[0m training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtraining\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/explain/explainer.py:115\u001b[0m, in \u001b[0;36mExplainer.get_prediction\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n\u001b[1;32m    114\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 115\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain(training)\n\u001b[1;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/tesis/gcnn_gdas/exploration/model_exploration/../../src/models/base_model.py:197\u001b[0m, in \u001b[0;36mbase_model.forward\u001b[0;34m(self, data, supervision_types)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,data,supervision_types):\n\u001b[0;32m--> 197\u001b[0m     x \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mx_dict\n\u001b[1;32m    198\u001b[0m     adj_t \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39madj_t_dict\n\u001b[1;32m    199\u001b[0m     edge_label_index \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39medge_label_index_dict\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'x_dict'"
     ]
    }
   ],
   "source": [
    "model_config = ModelConfig(\n",
    "    mode='binary_classification',\n",
    "    task_level='edge',\n",
    "    return_type='raw',\n",
    ")\n",
    "\n",
    "explainer = Explainer(\n",
    "    model,  # It is assumed that model outputs a single tensor.\n",
    "    algorithm=CaptumExplainer('IntegratedGradients'),\n",
    "    explanation_type='model',\n",
    "    node_mask_type='attributes',\n",
    "    edge_mask_type='object',\n",
    "    model_config = model_config\n",
    ")\n",
    "\n",
    "hetero_explanation = explainer(\n",
    "    train_data.x_dict,\n",
    "    train_data.edge_index_dict,\n",
    "    index=torch.tensor([1, 3]),\n",
    ")\n",
    "print(hetero_explanation.edge_mask_dict)\n",
    "print(hetero_explanation.node_mask_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
