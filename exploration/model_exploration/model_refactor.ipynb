{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os.path as osp\n",
    "from torch_geometric.explain import Explainer, CaptumExplainer,ModelConfig,GNNExplainer\n",
    "from torch_geometric.nn import SAGEConv, GATConv\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"../../data/processed/graph_data_nohubs/\"\n",
    "models_folder = \"../../data/models/\"\n",
    "experiments_folder = \"../../data/experiments/design_space_experiment/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def load_data(folder_path,load_test = False):\n",
    "    if load_test:\n",
    "        names = [\"train\",\"validation\",\"test\"]\n",
    "    else:\n",
    "        names = [\"train\",\"validation\"]\n",
    "    datasets = []\n",
    "    for name in names:\n",
    "        path = folder_path+name+\".pt\"\n",
    "        datasets.append(torch.load(path))\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "def initialize_features(data,feature,dim,inplace=False):\n",
    "    if inplace:\n",
    "        data_object = data\n",
    "    else:\n",
    "        data_object = copy.copy(data)\n",
    "    for nodetype, store in data_object.node_items():\n",
    "        if feature == \"random\":\n",
    "            data_object[nodetype].x = torch.rand(store[\"num_nodes\"],dim)\n",
    "        if feature == \"ones\":\n",
    "            data_object[nodetype].x = torch.ones(store[\"num_nodes\"],dim)\n",
    "    return data_object\n",
    "\n",
    "def save_model(model,folder_path,model_name):\n",
    "    date = datetime.datetime.now()\n",
    "    fdate = date.strftime(\"%d_%m_%y__\")\n",
    "    fname = f\"{model_name}_{fdate}\"\n",
    "    torch.save(model.state_dict(), f\"{folder_path}{fname}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = data_folder+\"split_dataset/\"\n",
    "original_train_data, original_val_data = load_data(path)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import SAGEConv,GATConv, to_hetero\n",
    "from torch_geometric.nn import HeteroDictLinear\n",
    "\n",
    "class inner_product_decoder(torch.nn.Module):\n",
    "    def forward(self,x_source,x_target,edge_index,apply_sigmoid=True):\n",
    "        nodes_src = x_source[edge_index[0]]\n",
    "        nodes_trg = x_target[edge_index[1]]\n",
    "        pred = (nodes_src * nodes_trg).sum(dim=-1)\n",
    "\n",
    "        if apply_sigmoid:\n",
    "            pred = torch.sigmoid(pred)\n",
    "\n",
    "        return pred\n",
    "\n",
    "class base_message_layer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model_params,hidden_layer=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Currently SageConv or GATConv, might have to modify this to support other Convs\n",
    "        conv_type = model_params[\"conv_type\"]\n",
    "        self.conv = layer_dict[conv_type]((-1,-1), model_params[\"hidden_channels\"],aggr=model_params[\"micro_aggregation\"],add_self_loops=False)\n",
    "        self.normalize = model_params[\"L2_norm\"]\n",
    "\n",
    "        post_conv_modules = []\n",
    "        if model_params[\"batch_norm\"]:\n",
    "            bn = torch.nn.BatchNorm1d(model_params[\"hidden_channels\"])\n",
    "            post_conv_modules.append(bn)\n",
    "        \n",
    "        if model_params[\"dropout\"] > 0:    \n",
    "            dropout = torch.nn.Dropout(p=model_params[\"dropout\"])\n",
    "            post_conv_modules.append(dropout)\n",
    "        \n",
    "        # No activation on final embedding layer\n",
    "        if hidden_layer:\n",
    "            activation = model_params[\"activation\"]()\n",
    "            post_conv_modules.append(activation)\n",
    "        \n",
    "        self.post_conv = torch.nn.Sequential(*post_conv_modules)\n",
    "\n",
    "    def forward(self, x:dict, edge_index:dict) -> dict:\n",
    "        x = self.conv(x,edge_index)\n",
    "        x = self.post_conv(x)\n",
    "        if self.normalize:\n",
    "            x = torch.nn.functional.normalize(x,2,-1)\n",
    "        return x\n",
    "\n",
    "class multilayer_message_passing(torch.nn.Module):\n",
    "    #TODO: consider input and output dims with skipcat. Currently the two supported convs auto-detect dimensions. Might have to modify this if i add more convs in the future.\n",
    "    def __init__(self,num_layers,model_params,metadata):\n",
    "        super().__init__()\n",
    "\n",
    "        self.skip = model_params[\"layer_connectivity\"]\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            hidden_layer = i != self.num_layers-1\n",
    "            layer = to_hetero(base_message_layer(model_params,hidden_layer),metadata,model_params[\"macro_aggregation\"])\n",
    "            self.add_module(f\"Layer_{i}\",layer)\n",
    "    \n",
    "    def hetero_skipsum(self,out: dict, identity:dict) -> dict:\n",
    "        x_transformed = {}\n",
    "        for key,out_val in out.items():\n",
    "            identity_val = identity[key]\n",
    "            transformed_val = out_val + identity_val\n",
    "            x_transformed[key] = transformed_val\n",
    "\n",
    "        return x_transformed\n",
    "\n",
    "    def hetero_skipcat(self,x: dict, x_i:dict) -> dict:\n",
    "        x_transformed = {}\n",
    "        for key,x_val in x.items():\n",
    "            x_i_val = x_i[key]\n",
    "            transformed_val = torch.cat([x_val,x_i_val],dim=-1)\n",
    "            x_transformed[key] = transformed_val\n",
    "\n",
    "        return x_transformed  \n",
    "    \n",
    "    def forward(self, x:dict, edge_index:dict) -> dict:\n",
    "        for i, layer in enumerate(self.children()):\n",
    "            identity = x\n",
    "            out = layer(x,edge_index)\n",
    "            if self.skip == \"skipsum\":\n",
    "                out = self.hetero_skipsum(out,identity)\n",
    "            elif self.skip == \"skipcat\" and i < self.num_layers -1:\n",
    "                out = self.hetero_skipcat(out,identity)\n",
    "        \n",
    "        return out \n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self,num_layers,in_dim,out_dim,model_params,hidden_dim=None):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dim = out_dim if hidden_dim is None else hidden_dim\n",
    "        \n",
    "        modules = []\n",
    "        if num_layers == 1:\n",
    "            if in_dim == -1:\n",
    "                modules.append(torch.nn.LazyLinear(out_dim))\n",
    "            else:\n",
    "                modules.append(torch.nn.Linear(in_dim,hidden_dim))\n",
    "        else:\n",
    "            for i in range(num_layers):\n",
    "                final_layer = i == num_layers-1\n",
    "                first_layer = i == 0\n",
    "                if first_layer:\n",
    "                    if in_dim == -1:\n",
    "                        modules.append(torch.nn.LazyLinear(hidden_dim))\n",
    "                        modules.append(model_params[\"activation\"]())\n",
    "                    else:\n",
    "                        modules.append(torch.nn.Linear(in_dim,hidden_dim))\n",
    "                        modules.append(model_params[\"activation\"]())                        \n",
    "                elif final_layer:\n",
    "                    modules.append(torch.nn.Linear(hidden_dim,out_dim))\n",
    "                else:\n",
    "                    modules.append(torch.nn.Linear(hidden_dim,hidden_dim))\n",
    "                    modules.append(model_params[\"activation\"]())\n",
    "        \n",
    "        self.model = torch.nn.Sequential(*modules)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class base_encoder(torch.nn.Module):\n",
    "    def __init__(self,model_params,metadata):\n",
    "        super().__init__()\n",
    "\n",
    "        self.has_pre_mlp = model_params[\"pre_process_layers\"] > 0\n",
    "        self.has_post_mlp = model_params[\"post_process_layers\"] > 0\n",
    "\n",
    "        if self.has_pre_mlp:\n",
    "            self.pre_mlp = to_hetero(MLP(model_params[\"pre_process_layers\"],-1,model_params[\"hidden_channels\"],model_params),metadata)\n",
    "        \n",
    "        self.message_passing = multilayer_message_passing(model_params[\"msg_passing_layers\"],model_params,metadata)\n",
    "\n",
    "        if self.has_post_mlp:\n",
    "            self.post_mlp = to_hetero(MLP(model_params[\"post_process_layers\"],model_params[\"hidden_channels\"],model_params[\"hidden_channels\"],model_params),metadata)\n",
    "    \n",
    "    def forward(self,x:dict,edge_index:dict) -> dict :\n",
    "        if self.has_pre_mlp:\n",
    "            x = self.pre_mlp(x)\n",
    "\n",
    "        x = self.message_passing(x,edge_index)\n",
    "        \n",
    "        if self.has_post_mlp:\n",
    "            x = self.post_mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class base_model(torch.nn.Module):\n",
    "    def __init__(self, model_params,metadata):\n",
    "        super().__init__()\n",
    "\n",
    "        default_model_params = {\n",
    "            \"hidden_channels\":32,\n",
    "            \"conv_type\":\"SAGEConv\",\n",
    "            \"batch_norm\": True,\n",
    "            \"dropout\":0,\n",
    "            \"activation\":torch.nn.LeakyReLU,\n",
    "            \"micro_aggregation\":\"mean\",\n",
    "            \"macro_aggregation\":\"mean\",\n",
    "            \"layer_connectivity\":None,\n",
    "            \"L2_norm\":False,\n",
    "            \"feature_dim\": 10,\n",
    "            \"pre_process_layers\":0,\n",
    "            \"msg_passing_layers\":2,\n",
    "            \"post_process_layers\":0,\n",
    "        }\n",
    "        \n",
    "        for arg in default_model_params:\n",
    "            if arg not in model_params:\n",
    "                model_params[arg] = default_model_params[arg]\n",
    "        \n",
    "        self.encoder = base_encoder(model_params,metadata)\n",
    "        self.decoder = inner_product_decoder()\n",
    "        self.loss_fn = torch.nn.BCELoss()\n",
    "    \n",
    "    def decode(self,x:dict,edge_label_index:dict,supervision_types):\n",
    "        pred_dict = {}\n",
    "        for edge_type in supervision_types:\n",
    "            edge_index = edge_label_index[edge_type]\n",
    "\n",
    "            src_type = edge_type[0]\n",
    "            trg_type = edge_type[2]\n",
    "\n",
    "            x_src = x[src_type]\n",
    "            x_trg = x[trg_type]\n",
    "\n",
    "            pred = self.decoder(x_src,x_trg,edge_index)\n",
    "\n",
    "            pred_dict[edge_type] = pred\n",
    "        \n",
    "        return pred_dict\n",
    "    \n",
    "    def encode(self,data):\n",
    "        x = data.x_dict\n",
    "        adj_t = data.adj_t_dict\n",
    "        edge_index = data.edge_index_dict\n",
    "\n",
    "        encodings = self.encoder(x,adj_t)\n",
    "        return encodings\n",
    "    \n",
    "    def forward(self,data,supervision_types):\n",
    "        x = data.x_dict\n",
    "        adj_t = data.adj_t_dict\n",
    "        edge_index = data.edge_index_dict\n",
    "        edge_label_index = data.edge_label_index_dict\n",
    "\n",
    "        x = self.encoder(x,adj_t)\n",
    "        pred = self.decode(x,edge_label_index,supervision_types)\n",
    "        return pred\n",
    "    \n",
    "    def loss(self, prediction_dict, label_dict):\n",
    "        loss = 0\n",
    "        num_types = len(prediction_dict.keys())\n",
    "        for edge_type,pred in prediction_dict.items():\n",
    "            y = label_dict[edge_type]\n",
    "            loss += self.loss_fn(pred, y.type(pred.dtype))\n",
    "        return loss/num_types\n",
    "\n",
    "layer_dict = {\n",
    "    \"GATConv\":GATConv,\n",
    "    \"SAGEConv\":SAGEConv\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base_model 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import SAGEConv,GATConv, to_hetero\n",
    "\n",
    "class inner_product_decoder(torch.nn.Module):\n",
    "    def forward(self,x_source,x_target,edge_index,apply_sigmoid=True):\n",
    "        nodes_src = x_source[edge_index[0]]\n",
    "        nodes_trg = x_target[edge_index[1]]\n",
    "        pred = (nodes_src * nodes_trg).sum(dim=-1)\n",
    "\n",
    "        if apply_sigmoid:\n",
    "            pred = torch.sigmoid(pred)\n",
    "\n",
    "        return pred\n",
    "\n",
    "class base_message_layer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model_params,hidden_layer=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Currently SageConv or GATConv, might have to modify this to support other Convs\n",
    "        conv_type = model_params[\"conv_type\"]\n",
    "        self.conv = layer_dict[conv_type]((-1,-1), model_params[\"hidden_channels\"],aggr=model_params[\"micro_aggregation\"],add_self_loops=False)\n",
    "        self.normalize = model_params[\"L2_norm\"]\n",
    "\n",
    "        post_conv_modules = []\n",
    "        if model_params[\"batch_norm\"]:\n",
    "            bn = torch.nn.BatchNorm1d(model_params[\"hidden_channels\"])\n",
    "            post_conv_modules.append(bn)\n",
    "        \n",
    "        if model_params[\"dropout\"] > 0:    \n",
    "            dropout = torch.nn.Dropout(p=model_params[\"dropout\"])\n",
    "            post_conv_modules.append(dropout)\n",
    "        \n",
    "        # No activation on final embedding layer\n",
    "        if hidden_layer:\n",
    "            activation = model_params[\"activation\"]()\n",
    "            post_conv_modules.append(activation)\n",
    "        \n",
    "        self.post_conv = torch.nn.Sequential(*post_conv_modules)\n",
    "\n",
    "    def forward(self, x:dict, edge_index:dict) -> dict:\n",
    "        x = self.conv(x,edge_index)\n",
    "        x = self.post_conv(x)\n",
    "        if self.normalize:\n",
    "            x = torch.nn.functional.normalize(x,2,-1)\n",
    "        return x\n",
    "\n",
    "# class multilayer_message_passing(torch.nn.Module):\n",
    "#     #TODO: consider input and output dims with skipcat. Currently the two supported convs auto-detect dimensions. Might have to modify this if i add more convs in the future.\n",
    "#     def __init__(self,num_layers,model_params):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.skip = model_params[\"layer_connectivity\"]\n",
    "#         self.num_layers = num_layers\n",
    "\n",
    "#         for i in range(self.num_layers):\n",
    "#             hidden_layer = i != self.num_layers-1\n",
    "#             layer = base_message_layer(model_params,hidden_layer)\n",
    "#             self.add_module(f\"Layer_{i}\",layer)\n",
    "    \n",
    "#     def forward(self, x:dict, edge_index:dict) -> dict:\n",
    "#         for i, layer in enumerate(self.children()):\n",
    "#             identity = x\n",
    "#             out = layer(x,edge_index)\n",
    "#             if self.skip == \"skipsum\":\n",
    "#                 out += identity\n",
    "#             elif self.skip == \"skipcat\" and i < self.num_layers -1:\n",
    "#                 out = torch.cat([out,identity],dim=-1)\n",
    "        \n",
    "#         return out \n",
    "\n",
    "class multilayer_message_passing(torch.nn.Module):\n",
    "    #TODO: consider input and output dims with skipcat. Currently the two supported convs auto-detect dimensions. Might have to modify this if i add more convs in the future.\n",
    "    def __init__(self,num_layers,model_params):\n",
    "        super().__init__()\n",
    "\n",
    "        self.skip = model_params[\"layer_connectivity\"]\n",
    "        self.num_layers = num_layers\n",
    "        self.normalize = model_params[\"L2_norm\"]\n",
    "        self.batch_norm = model_params[\"batch_norm\"]\n",
    "        self.droput = model_params[\"dropout\"]\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            hidden_layer = i != self.num_layers-1\n",
    "            layer = base_message_layer(model_params,hidden_layer)\n",
    "            self.add_module(f\"Layer_{i}\",layer)\n",
    "    \n",
    "    def forward(self, x:dict, edge_index:dict) -> dict:\n",
    "        for i, layer in enumerate(self.children()):\n",
    "            identity = x\n",
    "            out = layer(x,edge_index)\n",
    "            if self.skip == \"skipsum\":\n",
    "                out += identity\n",
    "            elif self.skip == \"skipcat\" and i < self.num_layers -1:\n",
    "                out = torch.cat([out,identity],dim=-1)\n",
    "        \n",
    "        return out \n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self,num_layers,in_dim,out_dim,activate_last,hidden_dim=None):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dim = out_dim if hidden_dim is None else hidden_dim\n",
    "        \n",
    "        modules = []\n",
    "        if num_layers == 1:\n",
    "            if in_dim == -1:\n",
    "                modules.append(torch.nn.LazyLinear(out_dim))\n",
    "            else:\n",
    "                modules.append(torch.nn.Linear(in_dim,hidden_dim))\n",
    "        else:\n",
    "            for i in range(num_layers):\n",
    "                final_layer = i == num_layers-1\n",
    "                first_layer = i == 0\n",
    "                if first_layer:\n",
    "                    if in_dim == -1:\n",
    "                        modules.append(torch.nn.LazyLinear(hidden_dim))\n",
    "                        modules.append(torch.nn.LeakyReLU())\n",
    "                    else:\n",
    "                        modules.append(torch.nn.Linear(in_dim,hidden_dim))\n",
    "                        modules.append(torch.nn.LeakyReLU())                        \n",
    "                elif final_layer:\n",
    "                    modules.append(torch.nn.Linear(hidden_dim,out_dim))\n",
    "                else:\n",
    "                    modules.append(torch.nn.Linear(hidden_dim,hidden_dim))\n",
    "                    modules.append(torch.nn.LeakyReLU())\n",
    "        \n",
    "        if activate_last:\n",
    "            modules.append(torch.nn.LeakyReLU())\n",
    "        \n",
    "        self.model = torch.nn.Sequential(*modules)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
    "        for layer in self.children():\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class hetero_MLP(torch.nn.Module):\n",
    "    def __init__(self,model_params,metadata,layer_type):\n",
    "        super().__init__()\n",
    "\n",
    "        node_types = metadata[0]\n",
    "        if layer_type == \"pre_mlp\":\n",
    "            self.in_dim = -1\n",
    "            self.num_layers = model_params[\"pre_process_layers\"]\n",
    "            self.activate_last = True\n",
    "            \n",
    "        elif layer_type == \"post_mlp\":\n",
    "            self.in_dim = model_params[\"hidden_channels\"]\n",
    "            self.num_layers = model_params[\"post_process_layers\"]\n",
    "            self.activate_last = False\n",
    "        \n",
    "        self.out_dim = model_params[\"hidden_channels\"]\n",
    "        self.mlps = torch.nn.ModuleDict({})\n",
    "\n",
    "        for node_type in node_types:\n",
    "            self.mlps[node_type] = MLP(self.num_layers,self.in_dim,self.out_dim,self.activate_last)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
    "        for mlp in self.mlps.values():\n",
    "            mlp.reset_parameters()\n",
    "        \n",
    "    def forward(self,x:dict):\n",
    "        out_dict = {}\n",
    "        for key, mlp in self.mlps.items():\n",
    "            if key in x:\n",
    "                out_dict[key] = mlp(x[key])\n",
    "        return out_dict\n",
    "\n",
    "class base_encoder(torch.nn.Module):\n",
    "    def __init__(self,model_params,metadata):\n",
    "        super().__init__()\n",
    "\n",
    "        self.has_pre_mlp = model_params[\"pre_process_layers\"] > 0\n",
    "        self.has_post_mlp = model_params[\"post_process_layers\"] > 0\n",
    "\n",
    "        if self.has_pre_mlp:\n",
    "            self.pre_mlp = hetero_MLP(model_params,metadata,\"pre_mlp\")\n",
    "        \n",
    "        self.message_passing = to_hetero(multilayer_message_passing(model_params[\"msg_passing_layers\"],model_params),metadata,aggr=model_params[\"macro_aggregation\"])\n",
    "\n",
    "        if self.has_post_mlp:\n",
    "            self.post_mlp = hetero_MLP(model_params,metadata,\"post_mlp\")\n",
    "    \n",
    "    def forward(self,x:dict,edge_index:dict) -> dict :\n",
    "        if self.has_pre_mlp:\n",
    "            x = self.pre_mlp(x)\n",
    "\n",
    "        x = self.message_passing(x,edge_index)\n",
    "        \n",
    "        if self.has_post_mlp:\n",
    "            x = self.post_mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class base_model(torch.nn.Module):\n",
    "    def __init__(self, model_params,metadata):\n",
    "        super().__init__()\n",
    "\n",
    "        default_model_params = {\n",
    "            \"hidden_channels\":32,\n",
    "            \"conv_type\":\"SAGEConv\",\n",
    "            \"batch_norm\": True,\n",
    "            \"dropout\":0,\n",
    "            \"activation\":torch.nn.LeakyReLU,\n",
    "            \"micro_aggregation\":\"mean\",\n",
    "            \"macro_aggregation\":\"mean\",\n",
    "            \"layer_connectivity\":None,\n",
    "            \"L2_norm\":False,\n",
    "            \"feature_dim\": 10,\n",
    "            \"pre_process_layers\":0,\n",
    "            \"msg_passing_layers\":2,\n",
    "            \"post_process_layers\":0,\n",
    "        }\n",
    "        \n",
    "        for arg in default_model_params:\n",
    "            if arg not in model_params:\n",
    "                model_params[arg] = default_model_params[arg]\n",
    "        \n",
    "        self.encoder = base_encoder(model_params,metadata)\n",
    "        self.decoder = inner_product_decoder()\n",
    "        self.loss_fn = torch.nn.BCELoss()\n",
    "    \n",
    "    def decode(self,x:dict,edge_label_index:dict,supervision_types):\n",
    "        pred_dict = {}\n",
    "        for edge_type in supervision_types:\n",
    "            edge_index = edge_label_index[edge_type]\n",
    "\n",
    "            src_type = edge_type[0]\n",
    "            trg_type = edge_type[2]\n",
    "\n",
    "            x_src = x[src_type]\n",
    "            x_trg = x[trg_type]\n",
    "\n",
    "            pred = self.decoder(x_src,x_trg,edge_index)\n",
    "\n",
    "            pred_dict[edge_type] = pred\n",
    "        \n",
    "        return pred_dict\n",
    "    \n",
    "    def encode(self,data):\n",
    "        x = data.x_dict\n",
    "        adj_t = data.adj_t_dict\n",
    "        edge_index = data.edge_index_dict\n",
    "\n",
    "        encodings = self.encoder(x,adj_t)\n",
    "        return encodings\n",
    "    \n",
    "    def forward(self,data,supervision_types):\n",
    "        x = data.x_dict\n",
    "        adj_t = data.adj_t_dict\n",
    "        edge_index = data.edge_index_dict\n",
    "        edge_label_index = data.edge_label_index_dict\n",
    "\n",
    "        x = self.encoder(x,adj_t)\n",
    "        pred = self.decode(x,edge_label_index,supervision_types)\n",
    "        return pred\n",
    "    \n",
    "    def loss(self, prediction_dict, label_dict):\n",
    "        loss = 0\n",
    "        num_types = len(prediction_dict.keys())\n",
    "        for edge_type,pred in prediction_dict.items():\n",
    "            y = label_dict[edge_type]\n",
    "            loss += self.loss_fn(pred, y.type(pred.dtype))\n",
    "        return loss/num_types\n",
    "\n",
    "layer_dict = {\n",
    "    \"GATConv\":GATConv,\n",
    "    \"SAGEConv\":SAGEConv\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "            \"hidden_channels\":32,\n",
    "            \"conv_type\":\"SAGEConv\",\n",
    "            \"batch_norm\": True,\n",
    "            \"dropout\":0.1,\n",
    "            \"activation\":torch.nn.LeakyReLU,\n",
    "            \"micro_aggregation\":\"sum\",\n",
    "            \"macro_aggregation\":\"mean\",\n",
    "            \"layer_connectivity\":None,\n",
    "            \"L2_norm\":True,\n",
    "            \"feature_dim\": 10,\n",
    "            \"pre_process_layers\":2,\n",
    "            \"msg_passing_layers\":2,\n",
    "            \"post_process_layers\":2,\n",
    "        }\n",
    "\n",
    "dblp_model = base_model(model_params,train_data.metadata())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dblp_model(train_data,[(\u001b[39m'\u001b[39;49m\u001b[39mauthor\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mto\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mpaper\u001b[39;49m\u001b[39m'\u001b[39;49m)])\n",
      "File \u001b[0;32m~/Documents/tesis/gcnn_gdas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[49], line 236\u001b[0m, in \u001b[0;36mbase_model.forward\u001b[0;34m(self, data, supervision_types)\u001b[0m\n\u001b[1;32m    233\u001b[0m edge_index \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39medge_index_dict\n\u001b[1;32m    234\u001b[0m edge_label_index \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39medge_label_index_dict\n\u001b[0;32m--> 236\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x,adj_t)\n\u001b[1;32m    237\u001b[0m pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode(x,edge_label_index,supervision_types)\n\u001b[1;32m    238\u001b[0m \u001b[39mreturn\u001b[39;00m pred\n",
      "File \u001b[0;32m~/Documents/tesis/gcnn_gdas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[49], line 170\u001b[0m, in \u001b[0;36mbase_encoder.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_pre_mlp:\n\u001b[1;32m    168\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_mlp(x)\n\u001b[0;32m--> 170\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmessage_passing(x,edge_index)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_post_mlp:\n\u001b[1;32m    173\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_mlp(x)\n",
      "File \u001b[0;32m~/Documents/tesis/gcnn_gdas/venv/lib/python3.10/site-packages/torch/fx/graph_module.py:662\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_wrapped\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 662\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrapped_call(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/tesis/gcnn_gdas/venv/lib/python3.10/site-packages/torch/fx/graph_module.py:281\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    280\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 281\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/Documents/tesis/gcnn_gdas/venv/lib/python3.10/site-packages/torch/fx/graph_module.py:271\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls_call(obj, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    270\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcls, obj)\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    273\u001b[0m     \u001b[39massert\u001b[39;00m e\u001b[39m.\u001b[39m__traceback__\n",
      "File \u001b[0;32m~/Documents/tesis/gcnn_gdas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m<eval_with_key>.29:28\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     26\u001b[0m layer_1__paper__to__conference \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayer_1\u001b[39m.\u001b[39mpaper__to__conference((x__paper, x__conference), edge_index__paper__to__conference);  edge_index__paper__to__conference \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     27\u001b[0m layer_1__term__to__paper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayer_1\u001b[39m.\u001b[39mterm__to__paper((x__term, x__paper), edge_index__term__to__paper);  edge_index__term__to__paper \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m layer_1__conference__to__paper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mLayer_1\u001b[39m.\u001b[39;49mconference__to__paper((x__conference, x__paper), edge_index__conference__to__paper);  edge_index__conference__to__paper \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     29\u001b[0m layer_1__paper__rev_to__author \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayer_1\u001b[39m.\u001b[39mpaper__rev_to__author((x__paper, x__author), edge_index__paper__rev_to__author);  edge_index__paper__rev_to__author \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     30\u001b[0m layer_1__author__rev_to__paper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayer_1\u001b[39m.\u001b[39mauthor__rev_to__paper((x__author, x__paper), edge_index__author__rev_to__paper);  x__author \u001b[39m=\u001b[39m edge_index__author__rev_to__paper \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/tesis/gcnn_gdas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[49], line 42\u001b[0m, in \u001b[0;36mbase_message_layer.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x:\u001b[39mdict\u001b[39m, edge_index:\u001b[39mdict\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m:\n\u001b[0;32m---> 42\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x,edge_index)\n\u001b[1;32m     43\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_conv(x)\n\u001b[1;32m     44\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize:\n",
      "File \u001b[0;32m~/Documents/tesis/gcnn_gdas/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/tesis/gcnn_gdas/venv/lib/python3.10/site-packages/torch_geometric/nn/conv/sage_conv.py:131\u001b[0m, in \u001b[0;36mSAGEConv.forward\u001b[0;34m(self, x, edge_index, size)\u001b[0m\n\u001b[1;32m    128\u001b[0m     x \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin(x[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mrelu(), x[\u001b[39m1\u001b[39m])\n\u001b[1;32m    130\u001b[0m \u001b[39m# propagate_type: (x: OptPairTensor)\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate(edge_index, x\u001b[39m=\u001b[39;49mx, size\u001b[39m=\u001b[39;49msize)\n\u001b[1;32m    132\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin_l(out)\n\u001b[1;32m    134\u001b[0m x_r \u001b[39m=\u001b[39m x[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/tesis/gcnn_gdas/venv/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:435\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    434\u001b[0m         edge_index, msg_aggr_kwargs \u001b[39m=\u001b[39m res\n\u001b[0;32m--> 435\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmessage_and_aggregate(edge_index, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmsg_aggr_kwargs)\n\u001b[1;32m    436\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message_and_aggregate_forward_hooks\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m    437\u001b[0m     res \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, (edge_index, msg_aggr_kwargs), out)\n",
      "File \u001b[0;32m~/Documents/tesis/gcnn_gdas/venv/lib/python3.10/site-packages/torch_geometric/nn/conv/sage_conv.py:150\u001b[0m, in \u001b[0;36mSAGEConv.message_and_aggregate\u001b[0;34m(self, adj_t, x)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(adj_t, SparseTensor):\n\u001b[1;32m    149\u001b[0m     adj_t \u001b[39m=\u001b[39m adj_t\u001b[39m.\u001b[39mset_value(\u001b[39mNone\u001b[39;00m, layout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 150\u001b[0m \u001b[39mreturn\u001b[39;00m spmm(adj_t, x[\u001b[39m0\u001b[39;49m], reduce\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maggr)\n",
      "File \u001b[0;32m~/Documents/tesis/gcnn_gdas/venv/lib/python3.10/site-packages/torch_geometric/utils/spmm.py:43\u001b[0m, in \u001b[0;36mspmm\u001b[0;34m(src, other, reduce)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`reduce` argument \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mreduce\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not supported\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(src, SparseTensor):\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mif\u001b[39;00m (torch_geometric\u001b[39m.\u001b[39mtyping\u001b[39m.\u001b[39mWITH_PT2 \u001b[39mand\u001b[39;00m other\u001b[39m.\u001b[39;49mdim() \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m     44\u001b[0m             \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m src\u001b[39m.\u001b[39mis_cuda()):\n\u001b[1;32m     45\u001b[0m         \u001b[39m# Use optimized PyTorch `torch.sparse.mm` path:\u001b[39;00m\n\u001b[1;32m     46\u001b[0m         csr \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mto_torch_sparse_csr_tensor()\n\u001b[1;32m     47\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msparse\u001b[39m.\u001b[39mmm(csr, other, reduce)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "dblp_model(train_data,[('author', 'to', 'paper')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "base_model(\n",
       "  (encoder): base_encoder(\n",
       "    (pre_mlp): hetero_MLP(\n",
       "      (mlps): ModuleDict(\n",
       "        (author): MLP(\n",
       "          (model): Sequential(\n",
       "            (0): LazyLinear(in_features=0, out_features=32, bias=True)\n",
       "            (1): LeakyReLU(negative_slope=0.01)\n",
       "            (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (3): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "        (paper): MLP(\n",
       "          (model): Sequential(\n",
       "            (0): LazyLinear(in_features=0, out_features=32, bias=True)\n",
       "            (1): LeakyReLU(negative_slope=0.01)\n",
       "            (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (3): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "        (term): MLP(\n",
       "          (model): Sequential(\n",
       "            (0): LazyLinear(in_features=0, out_features=32, bias=True)\n",
       "            (1): LeakyReLU(negative_slope=0.01)\n",
       "            (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (3): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "        (conference): MLP(\n",
       "          (model): Sequential(\n",
       "            (0): LazyLinear(in_features=0, out_features=32, bias=True)\n",
       "            (1): LeakyReLU(negative_slope=0.01)\n",
       "            (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (3): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (message_passing): GraphModule(\n",
       "      (Layer_0): base_message_layer(\n",
       "        (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "        (post_conv): Sequential(\n",
       "          (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "      (Layer_1): ModuleDict(\n",
       "        (author__to__paper): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (paper__to__author): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (paper__to__term): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (paper__to__conference): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (term__to__paper): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (conference__to__paper): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (paper__rev_to__author): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (author__rev_to__paper): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (term__rev_to__paper): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (conference__rev_to__paper): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (paper__rev_to__term): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (paper__rev_to__conference): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_mlp): hetero_MLP(\n",
       "      (mlps): ModuleDict(\n",
       "        (author): MLP(\n",
       "          (model): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (1): LeakyReLU(negative_slope=0.01)\n",
       "            (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (paper): MLP(\n",
       "          (model): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (1): LeakyReLU(negative_slope=0.01)\n",
       "            (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (term): MLP(\n",
       "          (model): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (1): LeakyReLU(negative_slope=0.01)\n",
       "            (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (conference): MLP(\n",
       "          (model): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (1): LeakyReLU(negative_slope=0.01)\n",
       "            (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): inner_product_decoder()\n",
       "  (loss_fn): BCELoss()\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dblp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def hits_at_k(y_true,x_prob,k,key) -> dict:\n",
    "    \"\"\"Dados los tensores x_prob y edge_label, calcula cuantas predicciones hizo correctamente en los primeros k puntajes.\n",
    "    x_prob es la predicción del modelo luego de aplicar sigmoid (sin redondear, osea, el puntaje crudo)\"\"\"\n",
    "\n",
    "    #ordeno los puntajes de mayor a menor\n",
    "    x_prob, indices = torch.sort(x_prob, descending=True)\n",
    "\n",
    "    #me quedo solo con los k mayor punteados\n",
    "    x_prob = x_prob[:k]\n",
    "    indices = indices[:k]\n",
    "\n",
    "    if any(x_prob < 0.5):\n",
    "      threshold_index = (x_prob < 0.5).nonzero()[0].item()\n",
    "      print(f\"Top {k} scores for {key} below classification threshold 0.5, threshold index: {threshold_index}\")\n",
    "\n",
    "    #busco que label tenían esas k preds\n",
    "    labels = y_true[indices]\n",
    "\n",
    "    #cuento cuantas veces predije uno positivo en el top k\n",
    "    hits = labels.sum().item()\n",
    "\n",
    "    return hits\n",
    "\n",
    "def train(model, optimizer, graph,supervision_types):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    preds = model(graph,supervision_types)\n",
    "    edge_label = graph.edge_label_dict\n",
    "    loss = model.loss(preds, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_val_loss(model,val_data,supervision_types):\n",
    "    model.eval()\n",
    "    preds = model(val_data,supervision_types)\n",
    "    edge_label = val_data.edge_label_dict\n",
    "    loss = model.loss(preds, edge_label)\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def get_metrics(y_true, x_pred):\n",
    "   acc = round(accuracy_score(y_true,x_pred),2)\n",
    "   ap = round(average_precision_score(y_true, x_pred),2)\n",
    "   roc_auc = round(roc_auc_score(y_true,x_pred),2)\n",
    "\n",
    "   return acc,ap ,roc_auc\n",
    "  \n",
    "@torch.no_grad()\n",
    "def test(model,data,supervision_types,metric):\n",
    "  model.eval()\n",
    "  preds = model(data,supervision_types)\n",
    "  edge_label = data.edge_label_dict\n",
    "  all_preds = []\n",
    "  all_true = []\n",
    "  for key,pred in preds.items():\n",
    "      pred_label = torch.round(pred)\n",
    "      ground_truth = edge_label[key]\n",
    "      all_preds.append(pred_label)\n",
    "      all_true.append(ground_truth)\n",
    "  total_predictions = torch.cat(all_preds, dim=0).cpu().numpy()\n",
    "  total_true = torch.cat(all_true, dim=0).cpu().numpy()\n",
    "  score = metric(total_true,total_predictions)\n",
    "  return score\n",
    "  \n",
    "\n",
    "@torch.no_grad()\n",
    "def full_test(model,data,supervision_types,k,global_score=True):\n",
    "  model.eval()\n",
    "  preds = model(data,supervision_types)\n",
    "  edge_label = data.edge_label_dict\n",
    "  metrics = {}\n",
    "\n",
    "  if global_score:\n",
    "    all_scores = []\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    for key,pred in preds.items():\n",
    "        pred_label = torch.round(pred)\n",
    "        ground_truth = edge_label[key]\n",
    "        all_scores.append(pred)\n",
    "        all_preds.append(pred_label)\n",
    "        all_true.append(ground_truth)\n",
    "\n",
    "    total_predictions = torch.cat(all_preds, dim=0)\n",
    "    total_true = torch.cat(all_true, dim=0)\n",
    "    total_scores = torch.cat(all_scores,dim=0)\n",
    "\n",
    "    acc, ap, roc_auc =  get_metrics(total_true.cpu().numpy(), total_predictions.cpu().numpy())\n",
    "    hits_k = hits_at_k(total_true,total_scores,k,\"all\")\n",
    "    metrics[\"all\"] = [acc,ap,roc_auc,hits_k]\n",
    "\n",
    "  else:\n",
    "    for key,pred in preds.items():\n",
    "        pred_label = torch.round(pred)\n",
    "        ground_truth = edge_label[key]\n",
    "        acc, ap, roc_auc = get_metrics(ground_truth.cpu().numpy(), pred_label.cpu().numpy())\n",
    "        hits_k = hits_at_k(ground_truth,pred,k,key)\n",
    "        metrics[key] = [acc,ap, roc_auc,hits_k]\n",
    "  \n",
    "  return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_stats(title, train_losses,val_losses, train_metric,val_metric,metric_str):\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(8,5))\n",
    "  ax2 = ax.twinx()\n",
    "\n",
    "  ax.set_xlabel(\"Training Epochs\")\n",
    "  ax2.set_ylabel(metric_str)\n",
    "  ax.set_ylabel(\"Loss\")\n",
    "\n",
    "  plt.title(title)\n",
    "  p1, = ax.plot(train_losses, \"b-\", label=\"training loss\")\n",
    "  p2, = ax2.plot(val_metric, \"r-\", label=f\"val {metric_str}\")\n",
    "  p3, = ax2.plot(train_metric, \"o-\", label=f\"train {metric_str}\")\n",
    "  p4, = ax.plot(val_losses,\"b--\",label=f\"validation loss\")\n",
    "  plt.legend(handles=[p1, p2, p3,p4],loc=2)\n",
    "  plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing base model on other datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import DBLP\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "path = \"../../data/DBLP\"\n",
    "dataset = DBLP(path)\n",
    "data = dataset[0]\n",
    "\n",
    "p_val = 0.1\n",
    "p_test = 0.1\n",
    "\n",
    "edge_types = [('author', 'to', 'paper'),('paper', 'to', 'term'),('paper', 'to', 'conference')]\n",
    "rev_edge_types = [('paper', 'to', 'author'),('term', 'to', 'paper'),('conference', 'to', 'paper')]\n",
    "\n",
    "data = T.ToUndirected()(data)\n",
    "split_transform = T.RandomLinkSplit(num_val=p_val, num_test=p_test, is_undirected=True, add_negative_train_samples=True, disjoint_train_ratio=0.2,edge_types=edge_types,rev_edge_types=rev_edge_types)\n",
    "transform_dataset = T.Compose([split_transform, T.ToSparseTensor(remove_edge_index=False)])\n",
    "\n",
    "train_data, val_data, test_data = transform_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def launch_experiment(model,train_set,val_set,params):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params[\"weight_decay\"])\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_scores = []\n",
    "    val_scores = []\n",
    "\n",
    "    metric = roc_auc_score\n",
    "    epochs = params[\"epochs\"]\n",
    "    supervision_types = params[\"supervision_types\"]\n",
    "\n",
    "    early_stopper = EarlyStopper(params[\"patience\"],params[\"delta\"])\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train(model,optimizer,train_set,supervision_types)\n",
    "        val_loss = get_val_loss(model,val_set,supervision_types)\n",
    "        train_score = test(model,train_set,supervision_types,metric)\n",
    "        val_score = test(model,val_set,supervision_types,metric)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_scores.append(train_score)\n",
    "        val_scores.append(val_score)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if epoch%50 == 0:\n",
    "            print(train_loss)\n",
    "        \n",
    "        if early_stopper.early_stop(val_loss):\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    val_auc = test(model,val_set,supervision_types,roc_auc_score)\n",
    "    curve_data = [train_losses,val_losses,train_scores,val_scores]\n",
    "\n",
    "    plot_training_stats(\"Trying explainers\", *curve_data,\"AUC\")\n",
    "    return model, val_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ingrid/Documents/tesis/gcnn_gdas/venv/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "multilayer_message_passing.__init__() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 17\u001b[0m\n\u001b[1;32m      1\u001b[0m model_params \u001b[39m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mhidden_channels\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m32\u001b[39m,\n\u001b[1;32m      3\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mconv_type\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mSAGEConv\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mpost_process_layers\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m1\u001b[39m,\n\u001b[1;32m     15\u001b[0m         }\n\u001b[0;32m---> 17\u001b[0m dblp_model \u001b[39m=\u001b[39m base_model(model_params,train_data\u001b[39m.\u001b[39;49mmetadata())\n",
      "Cell \u001b[0;32mIn[37], line 201\u001b[0m, in \u001b[0;36mbase_model.__init__\u001b[0;34m(self, model_params, metadata)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[39mif\u001b[39;00m arg \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m model_params:\n\u001b[1;32m    199\u001b[0m         model_params[arg] \u001b[39m=\u001b[39m default_model_params[arg]\n\u001b[0;32m--> 201\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m to_hetero(base_encoder(model_params,metadata),metadata,model_params[\u001b[39m\"\u001b[39m\u001b[39mmacro_aggregation\u001b[39m\u001b[39m\"\u001b[39m],input_map\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mnode\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39medge_index\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39medge\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[1;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder \u001b[39m=\u001b[39m inner_product_decoder()\n\u001b[1;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mBCELoss()\n",
      "Cell \u001b[0;32mIn[37], line 161\u001b[0m, in \u001b[0;36mbase_encoder.__init__\u001b[0;34m(self, model_params, metadata)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_pre_mlp:\n\u001b[1;32m    159\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_mlp \u001b[39m=\u001b[39m hetero_MLP(model_params,metadata,\u001b[39m\"\u001b[39m\u001b[39mpre_mlp\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmessage_passing \u001b[39m=\u001b[39m multilayer_message_passing(model_params[\u001b[39m\"\u001b[39;49m\u001b[39mmsg_passing_layers\u001b[39;49m\u001b[39m\"\u001b[39;49m],model_params,metadata)\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_post_mlp:\n\u001b[1;32m    164\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_mlp \u001b[39m=\u001b[39m hetero_MLP(model_params,metadata,\u001b[39m\"\u001b[39m\u001b[39mpost_mlp\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: multilayer_message_passing.__init__() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "model_params = {\n",
    "            \"hidden_channels\":32,\n",
    "            \"conv_type\":\"SAGEConv\",\n",
    "            \"batch_norm\": True,\n",
    "            \"dropout\":0.1,\n",
    "            \"activation\":torch.nn.LeakyReLU,\n",
    "            \"micro_aggregation\":\"sum\",\n",
    "            \"macro_aggregation\":\"mean\",\n",
    "            \"layer_connectivity\":None,\n",
    "            \"L2_norm\":True,\n",
    "            \"feature_dim\": 10,\n",
    "            \"pre_process_layers\":2,\n",
    "            \"msg_passing_layers\":2,\n",
    "            \"post_process_layers\":1,\n",
    "        }\n",
    "\n",
    "dblp_model = base_model(model_params,train_data.metadata())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "base_model(\n",
       "  (encoder): GraphModule(\n",
       "    (pre_mlp): ModuleDict(\n",
       "      (author): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): LazyLinear(in_features=0, out_features=32, bias=True)\n",
       "          (1): LeakyReLU(negative_slope=0.01)\n",
       "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (paper): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): LazyLinear(in_features=0, out_features=32, bias=True)\n",
       "          (1): LeakyReLU(negative_slope=0.01)\n",
       "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (term): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): LazyLinear(in_features=0, out_features=32, bias=True)\n",
       "          (1): LeakyReLU(negative_slope=0.01)\n",
       "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (conference): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): LazyLinear(in_features=0, out_features=32, bias=True)\n",
       "          (1): LeakyReLU(negative_slope=0.01)\n",
       "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (message_passing): ModuleDict(\n",
       "      (author__to__paper): multilayer_message_passing(\n",
       "        (Layer_0): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "        (Layer_1): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (paper__to__author): multilayer_message_passing(\n",
       "        (Layer_0): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "        (Layer_1): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (paper__to__term): multilayer_message_passing(\n",
       "        (Layer_0): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "        (Layer_1): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (paper__to__conference): multilayer_message_passing(\n",
       "        (Layer_0): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "        (Layer_1): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (term__to__paper): multilayer_message_passing(\n",
       "        (Layer_0): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "        (Layer_1): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conference__to__paper): multilayer_message_passing(\n",
       "        (Layer_0): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "        (Layer_1): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (paper__rev_to__author): multilayer_message_passing(\n",
       "        (Layer_0): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "        (Layer_1): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (author__rev_to__paper): multilayer_message_passing(\n",
       "        (Layer_0): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "        (Layer_1): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (term__rev_to__paper): multilayer_message_passing(\n",
       "        (Layer_0): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "        (Layer_1): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conference__rev_to__paper): multilayer_message_passing(\n",
       "        (Layer_0): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "        (Layer_1): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (paper__rev_to__term): multilayer_message_passing(\n",
       "        (Layer_0): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "        (Layer_1): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (paper__rev_to__conference): multilayer_message_passing(\n",
       "        (Layer_0): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "        (Layer_1): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_mlp): ModuleDict(\n",
       "      (author__to__paper): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (paper__to__author): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (paper__to__term): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (paper__to__conference): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (term__to__paper): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (conference__to__paper): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (paper__rev_to__author): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (author__rev_to__paper): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (term__rev_to__paper): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (conference__rev_to__paper): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (paper__rev_to__term): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (paper__rev_to__conference): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): inner_product_decoder()\n",
       "  (loss_fn): BCELoss()\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dblp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "base_model(\n",
       "  (encoder): GraphModule(\n",
       "    (pre_mlp): ModuleDict(\n",
       "      (author): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): LazyLinear(in_features=0, out_features=32, bias=True)\n",
       "          (1): LeakyReLU(negative_slope=0.01)\n",
       "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (paper): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): LazyLinear(in_features=0, out_features=32, bias=True)\n",
       "          (1): LeakyReLU(negative_slope=0.01)\n",
       "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (term): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): LazyLinear(in_features=0, out_features=32, bias=True)\n",
       "          (1): LeakyReLU(negative_slope=0.01)\n",
       "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (conference): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): LazyLinear(in_features=0, out_features=32, bias=True)\n",
       "          (1): LeakyReLU(negative_slope=0.01)\n",
       "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (message_passing): ModuleDict(\n",
       "      (author__to__paper): multilayer_message_passing(\n",
       "        (Layer_0): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "        (Layer_1): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (paper__to__author): multilayer_message_passing(\n",
       "        (Layer_0): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "        (Layer_1): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (paper__to__term): multilayer_message_passing(\n",
       "        (Layer_0): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "        (Layer_1): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (paper__to__conference): multilayer_message_passing(\n",
       "        (Layer_0): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "        (Layer_1): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (term__to__paper): multilayer_message_passing(\n",
       "        (Layer_0): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "        (Layer_1): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conference__to__paper): multilayer_message_passing(\n",
       "        (Layer_0): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "        (Layer_1): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (paper__rev_to__author): multilayer_message_passing(\n",
       "        (Layer_0): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "        (Layer_1): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (author__rev_to__paper): multilayer_message_passing(\n",
       "        (Layer_0): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "        (Layer_1): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (term__rev_to__paper): multilayer_message_passing(\n",
       "        (Layer_0): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "        (Layer_1): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conference__rev_to__paper): multilayer_message_passing(\n",
       "        (Layer_0): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "        (Layer_1): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (paper__rev_to__term): multilayer_message_passing(\n",
       "        (Layer_0): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "        (Layer_1): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (paper__rev_to__conference): multilayer_message_passing(\n",
       "        (Layer_0): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "            (2): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "        (Layer_1): base_message_layer(\n",
       "          (conv): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (post_conv): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_mlp): ModuleDict(\n",
       "      (author__to__paper): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (paper__to__author): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (paper__to__term): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (paper__to__conference): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (term__to__paper): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (conference__to__paper): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (paper__rev_to__author): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (author__rev_to__paper): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (term__rev_to__paper): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (conference__rev_to__paper): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (paper__rev_to__term): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (paper__rev_to__conference): MLP(\n",
       "        (model): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): inner_product_decoder()\n",
       "  (loss_fn): BCELoss()\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dblp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "base_model(\n",
       "  (encoder): base_encoder(\n",
       "    (pre_mlp): GraphModule(\n",
       "      (model): ModuleList(\n",
       "        (0): ModuleDict(\n",
       "          (author): LazyLinear(in_features=0, out_features=32, bias=True)\n",
       "          (paper): LazyLinear(in_features=0, out_features=32, bias=True)\n",
       "          (term): LazyLinear(in_features=0, out_features=32, bias=True)\n",
       "          (conference): LazyLinear(in_features=0, out_features=32, bias=True)\n",
       "        )\n",
       "        (1): ModuleDict(\n",
       "          (author): LeakyReLU(negative_slope=0.01)\n",
       "          (paper): LeakyReLU(negative_slope=0.01)\n",
       "          (term): LeakyReLU(negative_slope=0.01)\n",
       "          (conference): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (2): ModuleDict(\n",
       "          (author): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (paper): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (term): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (conference): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (message_passing): multilayer_message_passing(\n",
       "      (Layer_0): GraphModule(\n",
       "        (conv): ModuleDict(\n",
       "          (author__to__paper): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (paper__to__author): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (paper__to__term): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (paper__to__conference): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (term__to__paper): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (conference__to__paper): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (paper__rev_to__author): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (author__rev_to__paper): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (term__rev_to__paper): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (conference__rev_to__paper): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (paper__rev_to__term): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (paper__rev_to__conference): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "        )\n",
       "        (post_conv): ModuleList(\n",
       "          (0): ModuleDict(\n",
       "            (author): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (paper): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (term): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conference): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): ModuleDict(\n",
       "            (author): Dropout(p=0.1, inplace=False)\n",
       "            (paper): Dropout(p=0.1, inplace=False)\n",
       "            (term): Dropout(p=0.1, inplace=False)\n",
       "            (conference): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): ModuleDict(\n",
       "            (author): LeakyReLU(negative_slope=0.01)\n",
       "            (paper): LeakyReLU(negative_slope=0.01)\n",
       "            (term): LeakyReLU(negative_slope=0.01)\n",
       "            (conference): LeakyReLU(negative_slope=0.01)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (Layer_1): GraphModule(\n",
       "        (conv): ModuleDict(\n",
       "          (author__to__paper): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (paper__to__author): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (paper__to__term): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (paper__to__conference): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (term__to__paper): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (conference__to__paper): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (paper__rev_to__author): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (author__rev_to__paper): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (term__rev_to__paper): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (conference__rev_to__paper): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (paper__rev_to__term): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "          (paper__rev_to__conference): SAGEConv((-1, -1), 32, aggr=sum)\n",
       "        )\n",
       "        (post_conv): ModuleList(\n",
       "          (0): ModuleDict(\n",
       "            (author): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (paper): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (term): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conference): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): ModuleDict(\n",
       "            (author): Dropout(p=0.1, inplace=False)\n",
       "            (paper): Dropout(p=0.1, inplace=False)\n",
       "            (term): Dropout(p=0.1, inplace=False)\n",
       "            (conference): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_mlp): GraphModule(\n",
       "      (model): ModuleList(\n",
       "        (0): ModuleDict(\n",
       "          (author): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (paper): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (term): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (conference): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): inner_product_decoder()\n",
       "  (loss_fn): BCELoss()\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dblp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7022007703781128\n",
      "0.19571496546268463\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_params = {\n",
    "    'weight_decay': 1e-3,\n",
    "    'lr': 0.001,\n",
    "    'epochs':400,\n",
    "    \"patience\":5,\n",
    "    \"delta\":0.01,\n",
    "    \"supervision_types\":[('author', 'to', 'paper')]\n",
    "}\n",
    "trained_model, auc = launch_experiment(dblp_model,train_data,val_data,train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('author', 'to', 'paper'): [0.78, 0.7, 0.78, 197.0]}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_test(dblp_model,val_data,train_params[\"supervision_types\"],200,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.round(dblp_model(train_data,[(\"user\", \"rates\", \"movie\")])[(\"user\", \"rates\", \"movie\")])\n",
    "ground_truth = train_data.edge_label_dict[(\"user\", \"rates\", \"movie\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mmovie\u001b[0m={ x=[9742, 404] },\n",
       "  \u001b[1muser\u001b[0m={ x=[610, 610] },\n",
       "  \u001b[1m(user, rates, movie)\u001b[0m={\n",
       "    edge_index=[2, 80670],\n",
       "    edge_label=[80670],\n",
       "    edge_label_index=[2, 80670]\n",
       "  },\n",
       "  \u001b[1m(movie, rev_rates, user)\u001b[0m={ edge_index=[2, 80670] }\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
