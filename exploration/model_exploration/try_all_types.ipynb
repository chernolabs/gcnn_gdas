{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on Colab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ingrid/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  print('Running on Colab')\n",
    "  running_on_colab = True\n",
    "else:\n",
    "  print('Not running on Colab')\n",
    "  running_on_colab = False\n",
    "\n",
    "if running_on_colab:\n",
    "    print(torch.__version__)\n",
    "    !pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "    !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "    !pip install -q git+https://github.com/snap-stanford/deepsnap.git\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    filepath = '/content/drive/MyDrive/GCNN/graph_data/graphsage_prototype/'\n",
    "    data_folder = filepath+\"data/\"\n",
    "    models_folder = filepath+\"models/\"\n",
    "    experiments_folder = filepath+\"experiments/\"\n",
    "\n",
    "else:\n",
    "    data_folder = \"../../data/processed/graph_data_nohubs/\"\n",
    "    models_folder = \"../../data/models/\"\n",
    "    experiments_folder = \"../../data/experiments/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch_sparse import matmul\n",
    "import deepsnap.hetero_gnn\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_node_csv(path, index_col,type_col, **kwargs):\n",
    "    \"\"\"Returns node dataframe and a dict of mappings for each node type. \n",
    "    Each mapping maps from original df index to \"heterodata index\" { node_type : { dataframe_index : heterodata_index}}\"\"\"\n",
    "    df = pd.read_csv(path, **kwargs,index_col=index_col)\n",
    "    node_types = df[type_col].unique()\n",
    "    mappings_dict = dict()\n",
    "    for node_type in node_types:\n",
    "        mapping = {index: i for i, index in enumerate(df[df[type_col] == node_type].index.unique())}\n",
    "        mappings_dict[node_type] = mapping\n",
    "\n",
    "    return df,mappings_dict\n",
    "\n",
    "def load_edge_csv(path, src_index_col, dst_index_col, mappings, edge_type_col,src_type_col,dst_type_col, **kwargs):\n",
    "    \"\"\"Returns edge dataframe and a dict of edge indexes. Nodes are indexed according to the \"heterodata index\", using the node mappings from load_node_csv. \n",
    "    Edge indexes are tensors of shape [2, num_edges]. Dict is indexed by triplets of shape (src_type, edge_type, dst_type).\"\"\"\n",
    "    df = pd.read_csv(path, **kwargs)\n",
    "    df[\"edge_triple\"] = list(zip(df[src_type_col],df[edge_type_col], df[dst_type_col]))\n",
    "    edge_triplets = df[\"edge_triple\"].unique()\n",
    "\n",
    "    edge_index_dict = dict()\n",
    "    for edge_triplet in edge_triplets:\n",
    "\n",
    "        sub_df = df[df.edge_triple == edge_triplet]\n",
    "        src_type,edge_type,dst_type = edge_triplet\n",
    "\n",
    "        src_mapping = mappings[src_type]\n",
    "        dst_mapping = mappings[dst_type]\n",
    "\n",
    "        src = [src_mapping[index] for index in sub_df[src_index_col]]\n",
    "        dst = [dst_mapping[index] for index in sub_df[dst_index_col]]\n",
    "        edge_index = torch.tensor([src, dst])\n",
    "        edge_index_dict[edge_triplet] = edge_index\n",
    "\n",
    "    return df, edge_index_dict\n",
    "\n",
    "def create_heterodata(node_map, edge_index):\n",
    "    \"\"\"Initializes HeteroData object from torch_geometric and creates corresponding nodes and edges, without any features\"\"\"\n",
    "    data = HeteroData()\n",
    "    for node_type,vals in node_map.items():\n",
    "        # Initialize all node types without features\n",
    "        data[node_type].num_nodes = len(vals)\n",
    "    \n",
    "    for edge_triplet, index in edge_index.items():\n",
    "        src_type, edge_type, dst_type = edge_triplet\n",
    "        data[src_type, edge_type, dst_type].edge_index = index\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_reverse_types(edge_types):\n",
    "    newlist = []\n",
    "    for edge in edge_types:\n",
    "        rev = tuple(reversed(edge))\n",
    "        if rev != edge:\n",
    "            if edge not in newlist:\n",
    "                newlist.append(rev)\n",
    "        else:\n",
    "            newlist.append(rev)\n",
    "\n",
    "    reversed_newlist = [tuple(reversed(edge)) for edge in newlist]\n",
    "\n",
    "    return newlist, reversed_newlist\n",
    "\n",
    "def initialize_features(data_object,feature,dim):\n",
    "    for nodetype, store in data_object.node_items():\n",
    "        if feature == \"random\":\n",
    "            data_object[nodetype].x = torch.rand(store[\"num_nodes\"],dim)\n",
    "        if feature == \"ones\":\n",
    "            data_object[nodetype].x = torch.ones(store[\"num_nodes\"],dim)\n",
    "    return data_object"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiet = True\n",
    "\n",
    "def talk(msg, quiet=quiet):\n",
    "    if not quiet:\n",
    "        print(msg)\n",
    "\n",
    "def generate_convs(hetero_graph, conv, hidden_size, first_layer=False):\n",
    "    convs = {}\n",
    "\n",
    "    msg_types = hetero_graph.edge_types\n",
    "    for key in msg_types:\n",
    "        if first_layer:\n",
    "            dst_feature_dim = hetero_graph.num_node_features[key[2]]\n",
    "            src_feature_dim = hetero_graph.num_node_features[key[0]]\n",
    "            convs[key] = conv(src_feature_dim, dst_feature_dim, hidden_size)\n",
    "        else:\n",
    "            convs[key] = conv(hidden_size, hidden_size, hidden_size)\n",
    "\n",
    "    return convs\n",
    "\n",
    "def hetero_apply_function(x: dict,func) -> dict:\n",
    "    \"\"\"X es el diccionario de node embeddings o features, {node_type: tensor}.\n",
    "    Aplica func a cada entrada del diccionario, devuelve un dict de la misma forma.\"\"\"\n",
    "    x_transformed = {}\n",
    "    for key,val in x.items():\n",
    "        transformed_val = func(val)\n",
    "        x_transformed[key] = transformed_val\n",
    "    \n",
    "    return x_transformed\n",
    "\n",
    "class HeteroGNNConv(pyg_nn.MessagePassing):\n",
    "    def __init__(self, in_channels_src, in_channels_dst, out_channels):\n",
    "        super().__init__(aggr=\"mean\")\n",
    "\n",
    "        self.in_channels_src = in_channels_src\n",
    "        self.in_channels_dst = in_channels_dst\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.lin_dst = nn.Linear(in_channels_dst, out_channels)\n",
    "        self.lin_src = nn.Linear(in_channels_src, out_channels)\n",
    "        self.lin_update = nn.Linear(2*out_channels, out_channels)\n",
    "\n",
    "    def forward(self,node_feature_src, node_feature_dst,edge_index):\n",
    "        talk(\"HeteroGNN forward\")\n",
    "        talk(f\"Node feature src shape: {node_feature_src.shape}, Node feature dst shape: {node_feature_dst.shape}, edge index shape: {edge_index.sparse_sizes()}\")\n",
    "        out = self.propagate(edge_index, node_feature_src=node_feature_src,node_feature_dst=node_feature_dst)\n",
    "        return out\n",
    "\n",
    "    def message_and_aggregate(self, edge_index, node_feature_src):\n",
    "        talk(\"HeteroGNN msg and agg\")\n",
    "        talk(f\"node_feature src shape: {node_feature_src.shape}\")\n",
    "        out = matmul(edge_index, node_feature_src, reduce=self.aggr)\n",
    "        return out\n",
    "\n",
    "    def update(self, aggr_out, node_feature_dst):\n",
    "        talk(\"HeteroGNN update\")\n",
    "        talk(f\"Aggr_out shape: {aggr_out.shape}\")\n",
    "        talk(f\"Dst feature shape: {node_feature_dst.shape}\")\n",
    "        dst_msg = self.lin_dst(node_feature_dst)\n",
    "        src_msg = self.lin_src(aggr_out)\n",
    "\n",
    "        talk(f\"Concat: dst_msg shape: {dst_msg.shape}, src_msg shape: {src_msg.shape}\")\n",
    "        full_msg = torch.concat((dst_msg, src_msg), dim=1)\n",
    "\n",
    "        talk(f\"Full msg shape: {full_msg.shape}\")\n",
    "        out = self.lin_update(full_msg)\n",
    "\n",
    "        talk(f\"After update shape: {out.shape}\")\n",
    "        return out\n",
    "\n",
    "\n",
    "class HeteroGNNWrapperConv(deepsnap.hetero_gnn.HeteroConv):\n",
    "    def __init__(self, convs, aggr=\"mean\"):\n",
    "        super().__init__(convs, None)\n",
    "        self.aggr = aggr\n",
    "\n",
    "        # Map the index and message type\n",
    "        self.mapping = {}\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()\n",
    "\n",
    "    def forward(self, node_features, edge_indices):\n",
    "        talk(\"\\n ------ Wrapper forward ------ \")\n",
    "        message_type_emb = {}\n",
    "\n",
    "        for message_key, adj in edge_indices.items():\n",
    "            talk(f\"\\n{message_key}\\n\")\n",
    "            src_type, edge_type, dst_type = message_key\n",
    "            node_feature_src = node_features[src_type]\n",
    "            node_feature_dst = node_features[dst_type]\n",
    "\n",
    "            message_type_emb[message_key] = self.convs[message_key](node_feature_src,node_feature_dst,adj)\n",
    "\n",
    "        # {dst: [] for src, type, dst in message_type.emb.keys()}\n",
    "        # {tipo de nodo: [lista de embeddings obtenidos]}\n",
    "        node_emb = {dst: [] for _, _, dst in message_type_emb.keys()}\n",
    "        mapping = {}\n",
    "\n",
    "        for (src, edge_type, dst), item in message_type_emb.items():\n",
    "            #esto es para saber que indice es cada terna/msg type\n",
    "            mapping[len(node_emb[dst])] = (src, edge_type, dst)\n",
    "\n",
    "            #Agrego el embedding de la terna (src,type,dst) al la lista de embeddings de dst\n",
    "            node_emb[dst].append(item)\n",
    "\n",
    "        self.mapping = mapping\n",
    "\n",
    "        #Ahora hago aggregation sobre las listas de embeddings, para cada tipo de nodo DST\n",
    "        talk(\"\\n------ Wrapper agg ------\")\n",
    "        for node_type, embs in node_emb.items():\n",
    "            talk(f\"\\nAggregate {node_type} embeddings\")\n",
    "\n",
    "            # Si hay un solo embedding en la lista, me quedo con ese solito\n",
    "            if len(embs) == 1:\n",
    "                talk(f\"Num embeddings = 1, no AGG needed\")\n",
    "                node_emb[node_type] = embs[0]\n",
    "            \n",
    "            #Si hay más de uno hago aggregation\n",
    "            else:\n",
    "                node_emb[node_type] = self.aggregate(embs)\n",
    "        return node_emb\n",
    "\n",
    "    def aggregate(self, xs):\n",
    "        # Tomo la lista de embeddings para cada tipo de nodo y los \"agrego\". En este caso solo los promedio\n",
    "        # Stackeo los embeddings\n",
    "        talk(f\"Num embeddings: {len(xs)}\")\n",
    "        talk(f\"Shape embeddings: {[e.shape for e in xs]}\")\n",
    "        stacked = torch.stack(xs, dim=0)\n",
    "        talk(f\"Stacked shape: {stacked.shape}\")\n",
    "        out = torch.mean(stacked,dim=0)\n",
    "        talk(f\"Final aggregated shape: {out.shape}\")\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hetero_graph, pred_mode, hidden_size=32, aggr=\"mean\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.aggr = aggr\n",
    "        self.pred_mode = pred_mode\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bns1 = torch.nn.ModuleDict()\n",
    "        self.relus1 = torch.nn.ModuleDict()\n",
    "        self.loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        # if head==\"distmult\":\n",
    "        #   self.distmult_head = distmult_head(hetero_graph,self.hidden_size)\n",
    "\n",
    "        convs1 = generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=True)\n",
    "        convs2 = generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=False)\n",
    "        self.convs1 = HeteroGNNWrapperConv(convs1, aggr=self.aggr)\n",
    "        self.convs2 = HeteroGNNWrapperConv(convs2, aggr=self.aggr)\n",
    "        for node_type in hetero_graph.node_types:\n",
    "            self.bns1[node_type] = torch.nn.BatchNorm1d(self.hidden_size)\n",
    "            self.relus1[node_type] = torch.nn.LeakyReLU()\n",
    "    \n",
    "    def encode(self,graph):\n",
    "        talk(\" ------ ENCODER ------ \")\n",
    "        x = {k:v[\"x\"] for (k,v) in graph.node_items()}\n",
    "        adj = {k:v[\"adj_t\"] for (k,v) in graph.edge_items()}\n",
    "        \n",
    "        talk(\"Conv 1\")\n",
    "        x = self.convs1(x, edge_indices=adj)\n",
    "\n",
    "        talk(\"\\n BNS 1\")\n",
    "        x = deepsnap.hetero_gnn.forward_op(x, self.bns1)\n",
    "\n",
    "        talk(\"\\n Relu 1\")\n",
    "        x = deepsnap.hetero_gnn.forward_op(x, self.relus1)\n",
    "\n",
    "        talk(\"\\n Conv 2\")\n",
    "        x = self.convs2(x, edge_indices=adj)\n",
    "\n",
    "        talk(\"\\n----------\")\n",
    "        talk(f\"Node embeddings done. Dimentions: {[(k,item.shape) for k,item in x.items()]}\")\n",
    "        talk(\"---------\")\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def decode_train(self,x,graph):\n",
    "        supervision_types = [item[0] for item in graph.edge_items() if \"edge_label_index\" in item[1].keys()]\n",
    "        edge_label_index = {k:v[\"edge_label_index\"] for (k,v) in graph.edge_items() if k in supervision_types}\n",
    "\n",
    "        talk(\"\\n ------ DECODER ------ \")\n",
    "        pred = {}\n",
    "        if self.pred_mode == \"all\":\n",
    "            for message_type, edge_index in edge_label_index.items():\n",
    "                talk(f\"\\n Decoding edge type: {message_type}\")\n",
    "                src_type = message_type[0]\n",
    "                trg_type = message_type[2]\n",
    "\n",
    "                x_source = x[src_type]\n",
    "                x_target = x[trg_type]\n",
    "\n",
    "                nodes_src = x_source[edge_index[0]]\n",
    "                nodes_trg = x_target[edge_index[1]]\n",
    "\n",
    "                talk(f\"\\n Multiplying shapes: {nodes_src.shape}, {nodes_trg.shape}\")\n",
    "                pred[message_type] = torch.sum(nodes_src * nodes_trg, dim=-1)\n",
    "\n",
    "        elif self.pred_mode == \"gda_only\":\n",
    "            keys = [edge for edge in supervision_types if \"gda\" in edge]\n",
    "            for message_type in keys:\n",
    "                talk(f\"\\n Decoding edge type: {message_type}\")\n",
    "                edge_index = edge_label_index[message_type]\n",
    "                src_type = message_type[0]\n",
    "                trg_type = message_type[2]\n",
    "\n",
    "                x_source = x[src_type]\n",
    "                x_target = x[trg_type]\n",
    "\n",
    "                nodes_src = x_source[edge_index[0]]\n",
    "                nodes_trg = x_target[edge_index[1]]\n",
    "                talk(f\"\\n Multiplying shapes: {nodes_src.shape}, {nodes_trg.shape}\")\n",
    "                pred[message_type] = torch.sum(nodes_src * nodes_trg, dim=-1)\n",
    "\n",
    "        return pred\n",
    "    \n",
    "    def decode_pred(self,x1,x2):\n",
    "        talk(f\"\\n Multiplying shapes: {x1.shape}, {x2.shape}\")\n",
    "        pred_adj = torch.matmul(x1, x2.t())\n",
    "        pred_adj = torch.sigmoid(pred_adj)\n",
    "\n",
    "        return pred_adj\n",
    "\n",
    "    def forward(self, graph):\n",
    "        x = self.encode(graph)\n",
    "        pred = self.decode_train(x,graph)\n",
    "        \n",
    "        return pred\n",
    "          \n",
    "    def loss(self, prediction_dict, ground_truth_dict):\n",
    "        loss = 0\n",
    "        num_types = len(prediction_dict.keys())\n",
    "        # sets = torch.tensor(len(pred.keys()))\n",
    "        for edge_type,pred in prediction_dict.items():\n",
    "            y = ground_truth_dict[edge_type]\n",
    "            loss += self.loss_fn(pred, y.type(pred.dtype))\n",
    "        return loss/num_types"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def hits_at_k(y_true,x_prob,k,key) -> dict:\n",
    "    \"\"\"Dados los tensores x_prob y edge_label, calcula cuantas predicciones hizo correctamente en los primeros k puntajes.\n",
    "    x_prob es la predicción del modelo luego de aplicar sigmoid (sin redondear, osea, el puntaje crudo)\"\"\"\n",
    "\n",
    "    #ordeno los puntajes de mayor a menor\n",
    "    x_prob, indices = torch.sort(x_prob, descending=True)\n",
    "\n",
    "    #me quedo solo con los k mayor punteados\n",
    "    x_prob = x_prob[:k]\n",
    "    indices = indices[:k]\n",
    "\n",
    "    if any(x_prob < 0.5):\n",
    "      threshold_index = (x_prob < 0.5).nonzero()[0].item()\n",
    "      print(f\"Top {k} scores for {key} below classification threshold 0.5, threshold index: {threshold_index}\")\n",
    "\n",
    "    #busco que label tenían esas k preds\n",
    "    labels = y_true[indices]\n",
    "\n",
    "    #cuento cuantas veces predije uno positivo en el top k\n",
    "    hits = labels.sum().item()\n",
    "\n",
    "    return hits\n",
    "\n",
    "def train(model, optimizer, graph, printb):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    preds = model(graph) # acá no paso por sigmoid porque mi loss_fn es BCE with logits, que aplica sigmoid internamente!\n",
    "    edge_label = {k:v[\"edge_label\"] for (k,v) in graph.edge_items() if \"edge_label\" in v.keys()}\n",
    "    loss = model.loss(preds, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if printb:\n",
    "        print(loss.item())\n",
    "    return loss.item()\n",
    "\n",
    "def get_metrics(y_true, x_pred):\n",
    "   acc = round(accuracy_score(y_true,x_pred),2)\n",
    "   ap = round(average_precision_score(y_true, x_pred),2)\n",
    "   roc_auc = round(roc_auc_score(y_true,x_pred),2)\n",
    "\n",
    "   return acc,ap ,roc_auc\n",
    "  \n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model,data,metric):\n",
    "  model.eval()\n",
    "  preds = model(data)\n",
    "  edge_label = {k:v[\"edge_label\"] for (k,v) in data.edge_items() if \"edge_label\" in v.keys()}\n",
    "  all_preds = []\n",
    "  all_true = []\n",
    "  for key,pred in preds.items():\n",
    "      probabilities = torch.sigmoid(pred)\n",
    "      pred_label = torch.round(probabilities)\n",
    "      ground_truth = edge_label[key]\n",
    "      all_preds.append(pred_label)\n",
    "      all_true.append(ground_truth)\n",
    "  total_predictions = torch.cat(all_preds, dim=0).cpu().numpy()\n",
    "  total_true = torch.cat(all_true, dim=0).cpu().numpy()\n",
    "  score = metric(total_true,total_predictions)\n",
    "  return score\n",
    "  \n",
    "\n",
    "@torch.no_grad()\n",
    "def full_test(model,data,k,global_score=True):\n",
    "  model.eval()\n",
    "  preds = model(data)\n",
    "  edge_label = {k:v[\"edge_label\"] for (k,v) in data.edge_items() if \"edge_label\" in v.keys()}\n",
    "  metrics = {}\n",
    "\n",
    "  if global_score:\n",
    "    all_scores = []\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    for key,pred in preds.items():\n",
    "        probabilities = torch.sigmoid(pred)\n",
    "        pred_label = torch.round(probabilities)\n",
    "        ground_truth = edge_label[key]\n",
    "        all_scores.append(probabilities)\n",
    "        all_preds.append(pred_label)\n",
    "        all_true.append(ground_truth)\n",
    "\n",
    "    total_predictions = torch.cat(all_preds, dim=0).cpu().numpy()\n",
    "    total_true = torch.cat(all_true, dim=0).cpu().numpy()\n",
    "    total_scores = torch.cat(all_scores,dim=0).cpu().numpy()\n",
    "\n",
    "    acc, ap, roc_auc =  get_metrics(total_true, total_predictions)\n",
    "    hits_k = hits_at_k(total_true,total_scores,k,\"all\")\n",
    "    metrics[\"all\"] = [acc,ap,roc_auc,hits_k]\n",
    "\n",
    "  else:\n",
    "    for key,pred in preds.items():\n",
    "        probabilities = torch.sigmoid(pred)\n",
    "        pred_label = torch.round(probabilities)\n",
    "        ground_truth = edge_label[key]\n",
    "        acc, ap, roc_auc = get_metrics(ground_truth.cpu().numpy(), pred_label.cpu().numpy())\n",
    "        hits_k = hits_at_k(ground_truth,probabilities,k,key)\n",
    "        metrics[key] = [acc,ap, roc_auc,hits_k]\n",
    "  \n",
    "  return metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight_decay': 0.001,\n",
       " 'pred_mode': 'all',\n",
       " 'num_features': 10,\n",
       " 'max_epochs': 400,\n",
       " 'lr': 0.01,\n",
       " 'hidden_size': 64,\n",
       " 'feature_type': 'random',\n",
       " 'aggr': 'sum'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"graphsage_prototype_best_31_03_23__07_49\"\n",
    "\n",
    "with open(models_folder+\"params_\"+model_name+\".pickle\", 'rb') as handle:\n",
    "    params = pickle.load(handle)\n",
    "\n",
    "params[\"pred_mode\"] = \"all\"\n",
    "params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load data from csv and create heterodata object\n",
    "node_data, node_map = load_node_csv(data_folder+\"nohub_graph_nodes.csv\",\"node_index\",\"node_type\")\n",
    "edge_data, edge_index = load_edge_csv(data_folder+\"nohub_graph_edge_data.csv\",\"x_index\",\"y_index\",node_map,\"edge_type\",\"x_type\",\"y_type\")\n",
    "data = create_heterodata(node_map,edge_index)\n",
    "\n",
    "#Split the dataset\n",
    "edge_types, rev_edge_types = get_reverse_types(data.edge_types)\n",
    "data = initialize_features(data,params[\"feature_type\"],params[\"num_features\"])\n",
    "split_transform = T.RandomLinkSplit(num_val=0.3, num_test=0.3, is_undirected=True, add_negative_train_samples=True, disjoint_train_ratio=0.2,edge_types=edge_types,rev_edge_types=rev_edge_types)\n",
    "transform_dataset = T.Compose([split_transform, T.ToSparseTensor(remove_edge_index=False),T.ToDevice(device)])\n",
    "\n",
    "train_data, val_data, test_data = transform_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_stats(title, losses, train_metric,val_metric,metric_str):\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  ax2 = ax.twinx()\n",
    "\n",
    "  ax.set_xlabel(\"Training Epochs\")\n",
    "  ax2.set_ylabel(\"Performance Metric\")\n",
    "  ax.set_ylabel(\"Loss\")\n",
    "\n",
    "  plt.title(title)\n",
    "  p1, = ax.plot(losses, \"b-\", label=\"training loss\")\n",
    "  p2, = ax2.plot(val_metric, \"r-\", label=f\"val {metric_str}\")\n",
    "  p3, = ax2.plot(train_metric, \"o-\", label=f\"train {metric_str}\")\n",
    "  plt.legend(handles=[p1, p2, p3])\n",
    "  plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HeteroGNN(data,params[\"pred_mode\"],params[\"hidden_size\"],params[\"aggr\"]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7044072151184082\n",
      "0.4958636164665222\n",
      "0.3989097476005554\n",
      "0.3634708821773529\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/try_all_types.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/try_all_types.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/try_all_types.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     printb \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/try_all_types.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m loss \u001b[39m=\u001b[39m train(model,optimizer,train_data,printb)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/try_all_types.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m train_score \u001b[39m=\u001b[39m test(model,train_data,metric)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/try_all_types.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m val_score \u001b[39m=\u001b[39m test(model,val_data,metric)\n",
      "\u001b[1;32m/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/try_all_types.ipynb Cell 17\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, graph, printb)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/try_all_types.ipynb#X23sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m edge_label \u001b[39m=\u001b[39m {k:v[\u001b[39m\"\u001b[39m\u001b[39medge_label\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m (k,v) \u001b[39min\u001b[39;00m graph\u001b[39m.\u001b[39medge_items() \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39medge_label\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m v\u001b[39m.\u001b[39mkeys()}\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/try_all_types.ipynb#X23sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mloss(preds, edge_label)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/try_all_types.ipynb#X23sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/try_all_types.ipynb#X23sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ingrid/Documents/tesis/gcnn_gdas/exploration/model_exploration/try_all_types.ipynb#X23sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mif\u001b[39;00m printb:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "train_scores = []\n",
    "val_scores = []\n",
    "metric = accuracy_score\n",
    "metric_name = \"Global accuracy\"\n",
    "\n",
    "for epoch in range(params[\"max_epochs\"]):\n",
    "    if epoch%10 == 0:\n",
    "        printb = True\n",
    "    else:\n",
    "        printb = False\n",
    "\n",
    "    loss = train(model,optimizer,train_data,printb)\n",
    "    train_score = test(model,train_data,metric)\n",
    "    val_score = test(model,val_data,metric)\n",
    "    losses.append(loss)\n",
    "    train_scores.append(train_score)\n",
    "    val_scores.append(val_score)\n",
    "\n",
    "plot_training_stats(\"GraphSAGE prototype training stats\",losses,train_scores,val_scores,metric_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,param_dict,folder_path,model_name):\n",
    "    date = datetime.datetime.now()\n",
    "    fdate = date.strftime(\"%d_%m_%y__%I_%M\")\n",
    "    fname = f\"{model_name}_{fdate}\"\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{folder_path}{fname}.pth\")\n",
    "\n",
    "    with open(f\"{folder_path}params_{fname}.pickle\", 'wb') as handle:\n",
    "        pickle.dump(param_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model,params,models_folder,\"graphsage_all_types_experiment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
