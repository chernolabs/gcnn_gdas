{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from matplotlib_venn import venn3, venn2\n",
    "import random\n",
    "import plotly.io as pio\n",
    "import os \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 16\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "#esto es para forzar a plt a poner fondos blancos en las figuras aunque el tema del notebook sea oscuro\n",
    "#plt.rcParams['axes.facecolor'] = 'white'\n",
    "#plt.rcParams['figure.facecolor'] = 'white'\n",
    "cmap = plt.get_cmap(\"tab10\")\n",
    "pio.templates.default = \"seaborn\"\n",
    "\n",
    "sns.set_style(\"darkgrid\", rc={'xtick.bottom': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed = \"../../../data/processed/\"\n",
    "data_interim = \"../../../data/interim/\"\n",
    "data_external = \"../../../data/external/\"\n",
    "tfidf_reports = \"../../../reports/tfidf/\"\n",
    "lsa_reports = \"../../../reports/lsa/\"\n",
    "\n",
    "graph_node_data = pd.read_csv(data_processed+\"graph_data/grafo_alternativo_CG_nodos.csv\")\n",
    "disease_attributes = pd.read_csv(data_interim+\"grafo_alternativo_disease_attributes.csv\")\n",
    "\n",
    "nodos_bert = graph_node_data.loc[graph_node_data.node_type == \"bert_group\",[\"node_index\",\"node_id\",\"node_name\",\"node_source\"]].copy()\n",
    "disease_attributes = pd.concat([disease_attributes,nodos_bert])\n",
    "\n",
    "enfermedades_en_dd = graph_node_data.loc[graph_node_data.degree_dd != 0, \"node_index\"].values\n",
    "disease_attributes = disease_attributes.set_index(\"node_index\").loc[enfermedades_en_dd].reset_index()\n",
    "\n",
    "disease_attributes = pd.merge(graph_node_data[[\"node_index\",\"comunidades_infomap\",\"comunidades_louvain\"]],disease_attributes,left_on=\"node_index\",right_on=\"node_index\",how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sparse_dataframe(matrix_path,row_path,column_path,cols_str=True):\n",
    "    mat = sparse.load_npz(matrix_path)\n",
    "    row = np.loadtxt(row_path)\n",
    "    if cols_str:\n",
    "        col = np.loadtxt(column_path, dtype=\"str\")\n",
    "    else:\n",
    "        col = np.loadtxt(column_path)\n",
    "        \n",
    "    df = pd.DataFrame.sparse.from_spmatrix(mat, index=row, columns=col)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargo archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analisis previos\n",
    "infomap_clusters = pd.read_pickle(\"../../../reports/tfidf/infomap_analysis_checkpoint.pkl\")\n",
    "louvain_clusters = pd.read_pickle(\"../../../reports/tfidf/louvain_analysis_checkpoint.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document-Term-Matrix  de clusters\n",
    "\n",
    "path_infomap = data_processed + \"tfidf_infomap/\"\n",
    "path_louvain = data_processed + \"tfidf_louvain/\"\n",
    "\n",
    "monograms_infomap = load_sparse_dataframe(path_infomap + \"matriz_tfidf_infomap_0.npz\",path_infomap + \"rows_tfidf_infomap_0.txt\", path_infomap+\"cols_tfidf_infomap_0.txt\")\n",
    "monograms_louvain = load_sparse_dataframe(path_louvain + \"matriz_tfidf_louvain_0.npz\",path_louvain + \"rows_tfidf_louvain_0.txt\", path_louvain+\"cols_tfidf_louvain_0.txt\")\n",
    "\n",
    "bigrams_infomap = load_sparse_dataframe(path_infomap + \"matriz_tfidf_infomap_1.npz\",path_infomap + \"rows_tfidf_infomap_1.txt\", path_infomap+\"cols_tfidf_infomap_1.txt\")\n",
    "bigrams_louvain = load_sparse_dataframe(path_louvain + \"matriz_tfidf_louvain_1.npz\",path_louvain + \"rows_tfidf_louvain_1.txt\", path_louvain+\"cols_tfidf_louvain_1.txt\")\n",
    "\n",
    "trigrams_infomap = load_sparse_dataframe(path_infomap + \"matriz_tfidf_infomap_2.npz\",path_infomap + \"rows_tfidf_infomap_2.txt\", path_infomap+\"cols_tfidf_infomap_2.txt\")\n",
    "trigrams_louvain = load_sparse_dataframe(path_louvain + \"matriz_tfidf_louvain_2.npz\",path_louvain + \"rows_tfidf_louvain_2.txt\", path_louvain+\"cols_tfidf_louvain_2.txt\")\n",
    "\n",
    "infomap_dtm = [monograms_infomap, bigrams_infomap, trigrams_infomap]\n",
    "louvain_dtm = [monograms_louvain, bigrams_louvain, trigrams_louvain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document-Term-Matrix de nodos\n",
    "path = data_processed + \"tfidf_nodos/\"\n",
    "monograms_dtm = load_sparse_dataframe(path+\"matriz_nodos_tfidf_0.npz\",path+\"rows_tfidf_nodos_0.txt\",path+\"cols_tfidf_nodos_0.txt\")\n",
    "bigrams_dtm = load_sparse_dataframe(path+\"matriz_nodos_tfidf_1.npz\",path+\"rows_tfidf_nodos_1.txt\",path+\"cols_tfidf_nodos_1.txt\")\n",
    "trigrams_dtm = load_sparse_dataframe(path+\"matriz_nodos_tfidf_2.npz\",path+\"rows_tfidf_nodos_2.txt\",path+\"cols_tfidf_nodos_2.txt\")\n",
    "\n",
    "document_term_matrix = [monograms_dtm, bigrams_dtm, trigrams_dtm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scree plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nodos = monograms_dtm.sparse.to_dense()\n",
    "\n",
    "svd = TruncatedSVD(n_components=2000)\n",
    "lsa = svd.fit(X_nodos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lsa.explained_variance_ratio_\n",
    "x = np.linspace(1,len(y),len(y))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.plot(x,y,\"o-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con .explained_variance_ratio_ vemos la fracción de información que aporta cada componente\n",
    "evr = lsa.explained_variance_ratio_\n",
    "cant_componentes = range(1, len(evr) + 1)\n",
    "\n",
    "# Calculamos el acumulado con la función cumsum de numpy \n",
    "varianza_acumulada = np.cumsum(evr)\n",
    "\n",
    "#Elijo un corte en 90% de varianza acumulada\n",
    "corte = np.nonzero(varianza_acumulada>0.8)[0][0]\n",
    "\n",
    "# Graficamos la fracción de varianza que aporta cada componente\n",
    "# y la información acumulada\n",
    "sns.set_theme()\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "ax.plot(cant_componentes, varianza_acumulada, \"-o\")\n",
    "ax.vlines(x=cant_componentes[corte], ymax=varianza_acumulada[corte], ymin=np.min(varianza_acumulada), linestyle='--', alpha=0.5)\n",
    "ax.hlines(y=varianza_acumulada[corte], xmax=cant_componentes[corte], xmin=np.min(cant_componentes), linestyle='--', alpha=0.5)\n",
    "ax.set_xlim(xmin=-100, xmax=2000)\n",
    "ax.set_ylabel('Fracción de varianza acumulada - Nodos')\n",
    "ax.set_xlabel('Componentes')\n",
    "ax.set_title(\"Varianza acumlada vs número de componentes - SVD\")\n",
    "print(f\"Alcanzo el 80% de varianza acumlada con {corte} componentes\")\n",
    "# fig.savefig(lsa_reports+\"evr_plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_infomap = infomap_dtm[0].sparse.to_dense()\n",
    "\n",
    "svd_infomap = TruncatedSVD(n_components=1000)\n",
    "lsa_infomap = svd_infomap.fit(X_infomap)\n",
    "\n",
    "X_louvain = louvain_dtm[0].sparse.to_dense()\n",
    "\n",
    "svd_louvain = TruncatedSVD(n_components=1000)\n",
    "lsa_louvain = svd_louvain.fit(X_louvain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con .explained_variance_ratio_ vemos la fracción de información que aporta cada componente\n",
    "evr = lsa_infomap.explained_variance_ratio_\n",
    "cant_componentes = range(1, len(evr) + 1)\n",
    "\n",
    "# Calculamos el acumulado con la función cumsum de numpy \n",
    "varianza_acumulada = np.cumsum(evr)\n",
    "\n",
    "#Elijo un corte en 90% de varianza acumulada\n",
    "corte = np.nonzero(varianza_acumulada>0.8)[0][0]\n",
    "\n",
    "# Graficamos la fracción de varianza que aporta cada componente\n",
    "# y la información acumulada\n",
    "sns.set_theme()\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "ax.plot(cant_componentes, varianza_acumulada, \"-o\")\n",
    "ax.vlines(x=cant_componentes[corte], ymax=varianza_acumulada[corte], ymin=np.min(varianza_acumulada), linestyle='--', alpha=0.5)\n",
    "ax.hlines(y=varianza_acumulada[corte], xmax=cant_componentes[corte], xmin=np.min(cant_componentes), linestyle='--', alpha=0.5)\n",
    "ax.set_xlim(xmin=-100, xmax=1000)\n",
    "ax.set_ylabel('Fracción de varianza acumulada - Clusters Infomap')\n",
    "ax.set_xlabel('Componentes')\n",
    "ax.set_title(\"Varianza acumlada vs número de componentes - SVD\")\n",
    "print(f\"Alcanzo el 90% de varianza acumlada con {corte} componentes\")\n",
    "# fig.savefig(lsa_reports+\"evr_plot.png\")\n",
    "\n",
    "\n",
    "# con .explained_variance_ratio_ vemos la fracción de información que aporta cada componente\n",
    "evr = lsa_louvain.explained_variance_ratio_\n",
    "cant_componentes = range(1, len(evr) + 1)\n",
    "\n",
    "# Calculamos el acumulado con la función cumsum de numpy \n",
    "varianza_acumulada = np.cumsum(evr)\n",
    "\n",
    "#Elijo un corte en 90% de varianza acumulada\n",
    "corte = np.nonzero(varianza_acumulada>0.8)[0][0]\n",
    "\n",
    "# Graficamos la fracción de varianza que aporta cada componente\n",
    "# y la información acumulada\n",
    "sns.set_theme()\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "ax.plot(cant_componentes, varianza_acumulada, \"-o\")\n",
    "ax.vlines(x=cant_componentes[corte], ymax=varianza_acumulada[corte], ymin=np.min(varianza_acumulada), linestyle='--', alpha=0.5)\n",
    "ax.hlines(y=varianza_acumulada[corte], xmax=cant_componentes[corte], xmin=np.min(cant_componentes), linestyle='--', alpha=0.5)\n",
    "ax.set_xlim(xmin=-100, xmax=1000)\n",
    "ax.set_ylabel('Fracción de varianza acumulada - Clusters Louvain')\n",
    "ax.set_xlabel('Componentes')\n",
    "ax.set_title(\"Varianza acumlada vs número de componentes - SVD\")\n",
    "print(f\"Alcanzo el 90% de varianza acumlada con {corte} componentes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corte_infomap = 477\n",
    "corte_louvain = 154"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armo el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_infomap = TruncatedSVD(n_components=corte_infomap)\n",
    "lsa_infomap = svd_infomap.fit(X_infomap)\n",
    "lsa_infomap_data = lsa_infomap.transform(X_infomap)\n",
    "\n",
    "svd_louvain = TruncatedSVD(n_components=corte_louvain)\n",
    "lsa_louvain = svd_louvain.fit(X_louvain)\n",
    "lsa_louvain_data = lsa_louvain.transform(X_louvain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most important words for each topic\n",
    "vocab_louvain = X_louvain.columns.values\n",
    "component_data = {}\n",
    "\n",
    "for i, comp in enumerate(lsa_louvain.components_):\n",
    "    #Tuplas de cada término con su valor en esa componente\n",
    "    vocab_comp = zip(vocab_louvain, comp)\n",
    "\n",
    "    #Las ordeno según el valor de la componente, de mayor a menor, veo las primeras 10\n",
    "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
    "    wordlist = [pair[0] for pair in sorted_words]\n",
    "    scorelist = [round(pair[1],3) for pair in sorted_words]\n",
    "    component_data[i] = {\"wordlist\":wordlist,\"scorelist\":scorelist}\n",
    "\n",
    "component_vocab_louvain = pd.DataFrame.from_dict(component_data,orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most important words for each topic\n",
    "vocab_infomap = X_infomap.columns.values\n",
    "component_data = {}\n",
    "\n",
    "for i, comp in enumerate(lsa_infomap.components_):\n",
    "    #Tuplas de cada término con su valor en esa componente\n",
    "    vocab_comp = zip(vocab_infomap, comp)\n",
    "\n",
    "    #Las ordeno según el valor de la componente, de mayor a menor, veo las primeras 10\n",
    "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
    "    wordlist = [pair[0] for pair in sorted_words]\n",
    "    scorelist = [round(pair[1],3) for pair in sorted_words]\n",
    "    component_data[i] = {\"wordlist\":wordlist,\"scorelist\":scorelist}\n",
    "\n",
    "component_vocab_infomap = pd.DataFrame.from_dict(component_data,orient=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=1400)\n",
    "lsa = svd.fit(X_nodos)\n",
    "\n",
    "lsa_data = lsa.transform(X_nodos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most important words for each topic\n",
    "vocab = X_nodos.columns.values\n",
    "component_data = {}\n",
    "\n",
    "for i, comp in enumerate(lsa.components_):\n",
    "    #Tuplas de cada término con su valor en esa componente\n",
    "    vocab_comp = zip(vocab, comp)\n",
    "\n",
    "    #Las ordeno según el valor de la componente, de mayor a menor, veo las primeras 10\n",
    "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
    "    wordlist = [pair[0] for pair in sorted_words]\n",
    "    scorelist = [round(pair[1],3) for pair in sorted_words]\n",
    "    component_data[i] = {\"wordlist\":wordlist,\"scorelist\":scorelist}\n",
    "\n",
    "component_vocab = pd.DataFrame.from_dict(component_data,orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# component_vocab.to_pickle(lsa_reports+\"component_vobab.pkl\")\n",
    "# np.savetxt(lsa_reports+\"vectorized_data.txt\",lsa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(vocab,lsa.components_[5]), key= lambda x:x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtengo la matriz de similaridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsa_data_sparse = sparse.csr_matrix(lsa_data)\n",
    "# sparse.save_npz(lsa_reports+\"vectorized_data.npz\", lsa_data_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsa_data_dense = lsa_data.toarray()\n",
    "# lsa_similarity_matrix = cosine_similarity(lsa_data_dense,lsa_data_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsa_similarity_matrix_sparse = sparse.csr_matrix(lsa_similarity_matrix)\n",
    "# sparse.save_npz(lsa_reports+\"lsa_similarity_matrix.npz\",lsa_similarity_matrix_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# component_vocab = pd.read_pickle(lsa_reports+\"component_vobab.pkl\")\n",
    "# lsa_data = sparse.load_npz(lsa_reports+\"vectorized_data.npz\")\n",
    "# lsa_similarity_matrix = sparse.load_npz(lsa_reports+\"lsa_similarity_matrix.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "def plot_component_vocab(component,component_vocab):\n",
    "    n = component\n",
    "    fig,ax = plt.subplots(figsize=(8,6))\n",
    "    sns.barplot(x=component_vocab.loc[n,\"wordlist\"], y=component_vocab.loc[n,\"scorelist\"], ax=ax)\n",
    "    ax.set_title(f\"Top 10 términos de la componente {n}\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "def wordcloud_component_vocab(component,component_vocab):\n",
    "    n = component\n",
    "    words = component_vocab.loc[n,\"wordlist\"]\n",
    "    scores = component_vocab.loc[n,\"scorelist\"]\n",
    "    wordcloud_dict = {word:score for (word,score) in zip(words,scores)}\n",
    "    wordcloud = WordCloud()\n",
    "    wordcloud.generate_from_frequencies(frequencies=wordcloud_dict)\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_vocab_infomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_vocab_louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_component_vocab(8,component_vocab_louvain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_component_vocab(8,component_vocab_louvain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_infomap_df = pd.DataFrame(lsa_infomap_data, index=X_infomap.index.values)\n",
    "lsa_louvain_df = pd.DataFrame(lsa_louvain_data, index=X_louvain.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_cluster = np.random.choice(X_infomap.index.values,1)[0]\n",
    "top_component = lsa_infomap_df.loc[random_cluster].idxmax()\n",
    "\n",
    "plot_component_vocab(top_component,component_vocab_infomap)\n",
    "wordcloud_component_vocab(top_component,component_vocab_infomap)\n",
    "disease_attributes[disease_attributes.comunidades_infomap == random_cluster]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodo = np.random.choice(range(lsa_data.shape[0]),1)[0]\n",
    "vector_lsa = lsa_data[nodo]\n",
    "top_10_components_idx = np.argsort(-vector_lsa)[0:10]\n",
    "top_10_components_score = vector_lsa[top_10_components_idx]\n",
    "node_data = disease_attributes.iloc[nodo][[\"node_name\",\"mondo_definition\",\"umls_description\",\"orphanet_definition\"]].values\n",
    "display(node_data)\n",
    "for component in top_10_components_idx[0:5]:\n",
    "    plot_component_vocab(component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vuelvo a ver similaridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_similarity(similarity_matrix, cluster, algoritmo):\n",
    "    nodos_cluster = disease_attributes[disease_attributes[algoritmo] == cluster].node_index.values\n",
    "    cluster_matrix = similarity_matrix.loc[nodos_cluster,nodos_cluster]\n",
    "    return round(np.mean(cluster_matrix.values), 2)\n",
    "\n",
    "def mean_similarity_triu(similarity_matrix, cluster, algoritmo):\n",
    "    nodos_cluster = disease_attributes[disease_attributes[algoritmo] == cluster].node_index.values\n",
    "    cluster_matrix = similarity_matrix.loc[nodos_cluster,nodos_cluster].values\n",
    "    return round(np.mean(np.triu(cluster_matrix,1)), 2)\n",
    "\n",
    "def get_similarity(document_term_matrix,df=True):\n",
    "    ids = document_term_matrix.index.values\n",
    "    scores = sparse.csr_matrix(document_term_matrix.values)\n",
    "    similarity_matrix = cosine_similarity(scores,scores,False)\n",
    "    if df:\n",
    "        similarity_matrix = pd.DataFrame.sparse.from_spmatrix(similarity_matrix, index=ids, columns=ids)\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = disease_attributes.node_index.values\n",
    "lsa_similarity_matrix_df = pd.DataFrame.sparse.from_spmatrix(lsa_similarity_matrix, index=ids, columns=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infomap_similarity = pd.Series({cluster:mean_similarity(lsa_similarity_matrix_df, cluster, \"comunidades_infomap\") for cluster in infomap_clusters.comunidad.values}, name=\"mean_similarity_lsa\")\n",
    "louvain_similarity = pd.Series({cluster:mean_similarity(lsa_similarity_matrix_df, cluster, \"comunidades_louvain\") for cluster in louvain_clusters.comunidad.values}, name=\"mean_similarity_lsa\")\n",
    "\n",
    "infomap_clusters = pd.merge(infomap_clusters,infomap_similarity, left_on=\"comunidad\",right_index=True)\n",
    "louvain_clusters = pd.merge(louvain_clusters,louvain_similarity, left_on=\"comunidad\",right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(16,6))\n",
    "fig.suptitle(\"Similaridad media entre nodos de clusters\")\n",
    "# ticks = np.logspace(0,tamaños_infomap.max(),10)\n",
    "\n",
    "sns.histplot(data=infomap_clusters,x=\"mean_similarity_lsa\", ax=ax[0])\n",
    "ax[0].set_xlabel(\"Similaridad media\")\n",
    "ax[0].set_title(\"Infomap\")\n",
    "\n",
    "sns.histplot(data=louvain_clusters,x=\"mean_similarity_lsa\", ax=ax[1])\n",
    "ax[1].set_xlabel(\"Similaridad media\")\n",
    "ax[1].set_title(\"Louvain\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
