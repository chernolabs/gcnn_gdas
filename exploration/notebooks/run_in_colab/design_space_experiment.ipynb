{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on Colab\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  print('Running on Colab')\n",
    "  running_on_colab = True\n",
    "else:\n",
    "  print('Not running on Colab')\n",
    "  running_on_colab = False\n",
    "\n",
    "if running_on_colab:\n",
    "    print(torch.__version__)\n",
    "    !pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "    !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "    !pip install -q git+https://github.com/snap-stanford/deepsnap.git\n",
    "    !pip install pyarrow\n",
    "    !pip install fastparquet\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    filepath = '/content/drive/MyDrive/GCNN/'\n",
    "    data_folder = filepath+\"graph_data/\"\n",
    "    experiments_folder = filepath+\"experiments/merged_types_experiment/\"\n",
    "\n",
    "else:\n",
    "    data_folder = \"../../../data/processed/graph_data_nohubs/merged_types/\"\n",
    "    experiments_folder = \"../../../data/experiments/design_space_merged_experiment/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base_model, training_utils\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import datetime\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = data_folder + \"split_dataset/\"\n",
    "original_train_data, original_val_data = training_utils.load_data(path)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a single experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(params, train_set, val_set):\n",
    "\n",
    "    # Initialize node features\n",
    "    train_set = training_utils.initialize_features(\n",
    "        train_set, params[\"feature_type\"], params[\"feature_dim\"])\n",
    "    val_set = training_utils.initialize_features(\n",
    "        val_set, params[\"feature_type\"], params[\"feature_dim\"])\n",
    "    train_set.to(device)\n",
    "    val_set.to(device)\n",
    "\n",
    "    # Initialize model\n",
    "    model = base_model.base_model(\n",
    "        params, train_set.metadata(), params[\"supervision_types\"])\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=params['lr'], weight_decay=params[\"weight_decay\"])\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_scores = []\n",
    "    val_scores = []\n",
    "\n",
    "    metric = roc_auc_score\n",
    "    epochs = params[\"epochs\"]\n",
    "\n",
    "    early_stopper = training_utils.EarlyStopper(\n",
    "        params[\"patience\"], params[\"delta\"])\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = training_utils.train(model, optimizer, train_set)\n",
    "        val_loss = training_utils.get_val_loss(model, val_set)\n",
    "\n",
    "        train_score = training_utils.test(model, train_set, metric)\n",
    "        val_score = training_utils.test(model, val_set, metric)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_scores.append(train_score)\n",
    "\n",
    "        val_scores.append(val_score)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if early_stopper.early_stop(val_loss):\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    val_auc = training_utils.test(model, val_set, roc_auc_score)\n",
    "    curve_data = [train_losses, val_losses, train_scores, val_scores]\n",
    "\n",
    "    return val_auc, model, curve_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a grid of experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "def perform_hyperparameter_search(param_grid, train_set, val_set):\n",
    "  \n",
    "  default = {\n",
    "      \"hidden_channels\":[32],\n",
    "      \"conv_type\":[\"SAGEConv\"],\n",
    "      \"batch_norm\": [True],\n",
    "      \"dropout\":[0.1],\n",
    "      \"micro_aggregation\":[\"mean\"],\n",
    "      \"macro_aggregation\":[\"mean\"],\n",
    "      \"layer_connectivity\":[None],\n",
    "      \"L2_norm\":[False],\n",
    "      \"pre_process_layers\":[0],\n",
    "      \"msg_passing_layers\":[2],\n",
    "      \"post_process_layers\":[0],\n",
    "      \"normalize_output\":[False],\n",
    "      \"jumping_knowledge\":[False],\n",
    "\n",
    "      \"feature_dim\":[10],\n",
    "      \"feature_type\":[\"random\"],\n",
    "      \"supervision_types\":[[('gene_protein', 'gda', 'disease')]],\n",
    "\n",
    "      'weight_decay': [1e-3],\n",
    "      'lr': [0.001],\n",
    "      'epochs':[400],\n",
    "      \"patience\":[10],\n",
    "      \"delta\":[0.1]\n",
    "  }\n",
    "\n",
    "  for arg in default:\n",
    "    if arg not in param_grid:\n",
    "      param_grid[arg] = default[arg]\n",
    "\n",
    "  grid = ParameterGrid(param_grid)\n",
    "\n",
    "  auc_results = []\n",
    "  models = []\n",
    "\n",
    "  for eid,params in enumerate(grid):\n",
    "    # Launch a training experiment using the current set of parameters\n",
    "    val_auc,current_model,curve_data = run_experiment(\n",
    "                   params,\n",
    "                   train_set,\n",
    "                   val_set)\n",
    "    \n",
    "    params[\"auc\"] = val_auc\n",
    "    params[\"curve_data\"] = curve_data\n",
    "\n",
    "    auc_results.append(params)\n",
    "    models.append(current_model)\n",
    "\n",
    "    print(f\"Validation AUC: {round(val_auc,2)}. Iteration: {eid+1} of {grid.__len__()}\")\n",
    "\n",
    "  return auc_results, models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run multiple grids of experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiple_grids(grid_list,train_data,val_data):\n",
    "    all_results = []\n",
    "    all_models = []\n",
    "\n",
    "    for i,grid in enumerate(grid_list):\n",
    "        print(f\"Experiment grid {i+1} of {len(grid_list)}\")\n",
    "        experiment_results, models = perform_hyperparameter_search(grid, train_data,val_data)\n",
    "        results_df = pd.DataFrame(experiment_results)\n",
    "        all_results.append(results_df)\n",
    "        all_models.append(models)\n",
    "\n",
    "    final_results = pd.concat(all_results).reset_index(drop=True)\n",
    "    final_models = list(itertools.chain(*all_models))\n",
    "\n",
    "    date = datetime.datetime.now()\n",
    "    fdate = date.strftime(\"%d_%m_%y__%H_%M_%S\")\n",
    "    fname = experiments_folder+\"experiment_\"+fdate+\".parquet\"\n",
    "    final_results.to_parquet(fname)\n",
    "\n",
    "    for i,model in enumerate(final_models):\n",
    "        model_name = f\"experiment_{i}\"\n",
    "        training_utils.save_model(model,experiments_folder,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_grid = {\n",
    "    \"hidden_channels\":[32],\n",
    "    \"conv_type\":[\"SAGEConv\"],\n",
    "    \"batch_norm\": [True],\n",
    "    \"dropout\":[0.1],\n",
    "    \"micro_aggregation\":[\"sum\"],\n",
    "    \"macro_aggregation\":[\"sum\"],\n",
    "    \"layer_connectivity\":[None],\n",
    "    \"L2_norm\":[True],\n",
    "    \"pre_process_layers\":[0],\n",
    "    \"msg_passing_layers\":[2],\n",
    "    \"post_process_layers\":[1],\n",
    "    \"normalize_output\":[False],\n",
    "    \"jumping_knowledge\":[False],\n",
    "\n",
    "    \"feature_dim\":[10],\n",
    "    \"feature_type\":[\"ones\"],\n",
    "    \"supervision_types\":[[('gene_protein', 'gda', 'disease')]],\n",
    "\n",
    "    'weight_decay': [1e-3],\n",
    "    'lr': [0.001],\n",
    "    'epochs':[400],\n",
    "    \"patience\":[10],\n",
    "    \"delta\":[0.1]\n",
    "}\n",
    "\n",
    "grid_list = []\n",
    "\n",
    "grid_1 = {\"hidden_channels\":[32,64,128],\"micro_aggregation\":[\"sum\",\"mean\",\"max\"],\"macro_aggregation\":[\"sum\",\"mean\",\"max\"],\"feature_dim\":[10,50,100],\"feature_type\":[\"ones\",\"random\"]}\n",
    "grid_list.append(default_grid|grid_1)\n",
    "\n",
    "grid_2 = {\"hidden_channels\":[32],\"conv_type\":[\"SAGEConv\",\"GATConv\"],\"micro_aggregation\":[\"sum\",\"mean\",\"max\"],\"macro_aggregation\":[\"sum\",\"mean\",\"max\"],\"feature_type\":[\"ones\",\"random\"]}\n",
    "grid_list.append(default_grid|grid_2)\n",
    "\n",
    "grid_3 = {\"layer_connectivity\":[None,\"skipsum\"],\"msg_passing_layers\":[2,3,4,5],\"jumping_knowledge\":[False,True]}\n",
    "grid_list.append(default_grid|grid_3)\n",
    "\n",
    "grid_4 = {\"L2_norm\":[True,False],\"conv_type\":[\"SAGEConv\",\"GATConv\"],\"normalize_output\":[True,False]}\n",
    "grid_list.append(default_grid|grid_4)\n",
    "\n",
    "grid_5 = {\"pre_process_layers\":[0,1,2],\"post_process_layers\":[0,1,2],\"msg_passing_layers\":[0,1,2],\"feature_type\":[\"ones\",\"random\"]}\n",
    "grid_list.append(default_grid|grid_5)\n",
    "\n",
    "grid_6 = {\"batch_norm\":[True,False],\"dropout\":[0,0.1,0.01]}\n",
    "grid_list.append(default_grid|grid_6)\n",
    "\n",
    "num_experiments = sum([len(list(itertools.chain(*grid.values()))) for grid in grid_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 159 experiments ...\n",
      "Experiment 1 of 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRunning \u001b[39m\u001b[39m{\u001b[39;00mnum_experiments\u001b[39m}\u001b[39;00m\u001b[39m experiments ...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m run_multiple_grids(grid_list,original_train_data,original_val_data)\n",
      "Cell \u001b[0;32mIn[21], line 7\u001b[0m, in \u001b[0;36mrun_multiple_grids\u001b[0;34m(grid_list, train_data, val_data)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m i,grid \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(grid_list):\n\u001b[1;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExperiment \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(grid_list)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     experiment_results, models \u001b[39m=\u001b[39m perform_hyperparameter_search(grid, train_data,val_data)\n\u001b[1;32m      8\u001b[0m     results_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(experiment_results)\n\u001b[1;32m      9\u001b[0m     all_results\u001b[39m.\u001b[39mappend(results_df)\n",
      "Cell \u001b[0;32mIn[6], line 41\u001b[0m, in \u001b[0;36mperform_hyperparameter_search\u001b[0;34m(param_grid, train_set, val_set)\u001b[0m\n\u001b[1;32m     37\u001b[0m models \u001b[39m=\u001b[39m []\n\u001b[1;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m eid,params \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(grid):\n\u001b[1;32m     40\u001b[0m   \u001b[39m# Launch a training experiment using the current set of parameters\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m   val_auc,current_model,curve_data \u001b[39m=\u001b[39m run_experiment(\n\u001b[1;32m     42\u001b[0m                  params,\n\u001b[1;32m     43\u001b[0m                  train_set,\n\u001b[1;32m     44\u001b[0m                  val_set)\n\u001b[1;32m     46\u001b[0m   params[\u001b[39m\"\u001b[39m\u001b[39mauc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m val_auc\n\u001b[1;32m     47\u001b[0m   params[\u001b[39m\"\u001b[39m\u001b[39mcurve_data\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m curve_data\n",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(params, train_set, val_set)\u001b[0m\n\u001b[1;32m     22\u001b[0m early_stopper \u001b[39m=\u001b[39m training_utils\u001b[39m.\u001b[39mEarlyStopper(params[\u001b[39m\"\u001b[39m\u001b[39mpatience\u001b[39m\u001b[39m\"\u001b[39m],params[\u001b[39m\"\u001b[39m\u001b[39mdelta\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m---> 24\u001b[0m     train_loss \u001b[39m=\u001b[39m training_utils\u001b[39m.\u001b[39;49mtrain(model,optimizer,train_set)\n\u001b[1;32m     25\u001b[0m     val_loss \u001b[39m=\u001b[39m training_utils\u001b[39m.\u001b[39mget_val_loss(model,val_set)\n\u001b[1;32m     27\u001b[0m     train_score \u001b[39m=\u001b[39m training_utils\u001b[39m.\u001b[39mtest(model,train_set,metric)\n",
      "File \u001b[0;32m~/Documents/tesis/gcnn_gdas/exploration/notebooks/run_in_colab/training_utils.py:41\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, data)\u001b[0m\n\u001b[1;32m     39\u001b[0m edge_label \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39medge_label_dict\n\u001b[1;32m     40\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mloss(preds, edge_label)\n\u001b[0;32m---> 41\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     42\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     44\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Documents/tesis/gcnn_gdas/venv/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/tesis/gcnn_gdas/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"Running {num_experiments} experiments ...\")\n",
    "run_multiple_grids(grid_list,original_train_data,original_val_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
